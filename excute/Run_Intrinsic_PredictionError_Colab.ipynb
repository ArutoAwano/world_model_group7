{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a6unHjN70oN"
      },
      "source": [
        "# Google Colabで予測誤差による内発的報酬での探索を実行 (Atariのみ)\n",
        "\n",
        "このノートブックは、`student_code.py`と`train_intrinsic.py`をその場で生成し、Atari環境で学習を実行します。\n",
        "これにより、ノートブック単体で完結して動作します。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jou6GUmm70oS"
      },
      "source": [
        "## 1. セットアップ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJGyE6Bn70oT",
        "outputId": "f9d90af1-b238-41ec-98d5-46448911abb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jan 11 07:23:05 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# GPUの確認\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmGE3h2l70oV",
        "outputId": "9da3cea5-ea04-4dc4-a970-e24f66d1e53c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rGet:2 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rGet:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.81)] [\r                                                                               \rHit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:7 https://cli.github.com/packages stable/main amd64 Packages [345 B]\n",
            "Get:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [2,233 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:12 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,864 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [69.2 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,969 kB]\n",
            "Get:17 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,592 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,411 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,600 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-backports/main amd64 Packages [83.9 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [37.2 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,637 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,205 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,289 kB]\n",
            "Fetched 38.4 MB in 7s (5,160 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "cmake is already the newest version (3.22.1-1ubuntu1.22.04.2).\n",
            "zlib1g-dev is already the newest version (1:1.2.11.dfsg-2ubuntu9.2).\n",
            "zlib1g-dev set to manually installed.\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "xvfb is already the newest version (2:21.1.4-2ubuntu1.7~22.04.16).\n",
            "The following additional packages will be installed:\n",
            "  freeglut3 gir1.2-ibus-1.0 libasound2-dev libdbus-1-dev libdecor-0-dev\n",
            "  libdrm-dev libegl-dev libegl1-mesa-dev libgbm-dev libgl-dev libgles-dev\n",
            "  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n",
            "  libglx-dev libibus-1.0-5 libibus-1.0-dev libopengl-dev libpciaccess-dev\n",
            "  libpulse-dev libpulse-mainloop-glib0 libsndio-dev libudev-dev libudev1\n",
            "  libwayland-bin libwayland-dev libxcursor-dev libxfixes-dev libxi-dev\n",
            "  libxinerama-dev libxkbcommon-dev libxrandr-dev libxt-dev libxv-dev\n",
            "  libxxf86vm-dev\n",
            "Suggested packages:\n",
            "  libasound2-doc libwayland-doc libxt-doc libgle3 python3-numpy python3-tk\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 gir1.2-ibus-1.0 libasound2-dev libdbus-1-dev libdecor-0-dev\n",
            "  libdrm-dev libegl-dev libegl1-mesa-dev libgbm-dev libgl-dev libgles-dev\n",
            "  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n",
            "  libglx-dev libibus-1.0-5 libibus-1.0-dev libopengl-dev libpciaccess-dev\n",
            "  libpulse-dev libpulse-mainloop-glib0 libsdl2-dev libsndio-dev libudev-dev\n",
            "  libwayland-bin libwayland-dev libxcursor-dev libxfixes-dev libxi-dev\n",
            "  libxinerama-dev libxkbcommon-dev libxrandr-dev libxt-dev libxv-dev\n",
            "  libxxf86vm-dev python3-opengl\n",
            "The following packages will be upgraded:\n",
            "  libudev1\n",
            "1 upgraded, 38 newly installed, 0 to remove and 80 not upgraded.\n",
            "Need to get 5,190 kB of archives.\n",
            "After this operation, 33.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev1 amd64 249.11-0ubuntu3.17 [76.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libibus-1.0-5 amd64 1.5.26-4 [183 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 gir1.2-ibus-1.0 amd64 1.5.26-4 [88.3 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libasound2-dev amd64 1.2.6.1-1ubuntu1 [110 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdbus-1-dev amd64 1.12.20-2ubuntu4.1 [188 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-bin amd64 1.20.0-1ubuntu0.1 [20.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-dev amd64 1.20.0-1ubuntu0.1 [69.5 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdecor-0-dev amd64 0.1.0-3build1 [5,544 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpciaccess-dev amd64 0.16-3 [21.9 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-dev amd64 2.4.113-2~ubuntu0.22.04.1 [292 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libegl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [11.1 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgbm-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [9,542 B]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libibus-1.0-dev amd64 1.5.26-4 [185 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpulse-mainloop-glib0 amd64 1:15.99.1+dfsg1-1ubuntu2.2 [12.4 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpulse-dev amd64 1:15.99.1+dfsg1-1ubuntu2.2 [75.6 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsndio-dev amd64 1.8.1-1.1 [17.8 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev-dev amd64 249.11-0ubuntu3.17 [20.7 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfixes-dev amd64 1:6.0.0-1 [12.2 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcursor-dev amd64 1:1.2.0-2build4 [28.2 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxi-dev amd64 2:1.8-1build1 [193 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxinerama-dev amd64 2:1.1.4-3 [8,104 B]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-dev amd64 1.4.0-1 [54.9 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxrandr-dev amd64 2:1.5.2-1build1 [26.7 kB]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxv-dev amd64 2:1.0.11-1build2 [33.4 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86vm-dev amd64 1:1.1.4-1build3 [13.9 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsdl2-dev amd64 2.0.20+dfsg-2ubuntu1.22.04.1 [1,767 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/universe amd64 python3-opengl all 3.1.5+dfsg-1 [605 kB]\n",
            "Fetched 5,190 kB in 3s (2,029 kB/s)\n",
            "Extracting templates from packages: 100%\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../libudev1_249.11-0ubuntu3.17_amd64.deb ...\n",
            "Unpacking libudev1:amd64 (249.11-0ubuntu3.17) over (249.11-0ubuntu3.12) ...\n",
            "Setting up libudev1:amd64 (249.11-0ubuntu3.17) ...\n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "(Reading database ... 121689 files and directories currently installed.)\n",
            "Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-6) ...\n",
            "Selecting previously unselected package libibus-1.0-5:amd64.\n",
            "Preparing to unpack .../01-libibus-1.0-5_1.5.26-4_amd64.deb ...\n",
            "Unpacking libibus-1.0-5:amd64 (1.5.26-4) ...\n",
            "Selecting previously unselected package gir1.2-ibus-1.0:amd64.\n",
            "Preparing to unpack .../02-gir1.2-ibus-1.0_1.5.26-4_amd64.deb ...\n",
            "Unpacking gir1.2-ibus-1.0:amd64 (1.5.26-4) ...\n",
            "Selecting previously unselected package libasound2-dev:amd64.\n",
            "Preparing to unpack .../03-libasound2-dev_1.2.6.1-1ubuntu1_amd64.deb ...\n",
            "Unpacking libasound2-dev:amd64 (1.2.6.1-1ubuntu1) ...\n",
            "Selecting previously unselected package libdbus-1-dev:amd64.\n",
            "Preparing to unpack .../04-libdbus-1-dev_1.12.20-2ubuntu4.1_amd64.deb ...\n",
            "Unpacking libdbus-1-dev:amd64 (1.12.20-2ubuntu4.1) ...\n",
            "Selecting previously unselected package libwayland-bin.\n",
            "Preparing to unpack .../05-libwayland-bin_1.20.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libwayland-bin (1.20.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libwayland-dev:amd64.\n",
            "Preparing to unpack .../06-libwayland-dev_1.20.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libwayland-dev:amd64 (1.20.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libdecor-0-dev:amd64.\n",
            "Preparing to unpack .../07-libdecor-0-dev_0.1.0-3build1_amd64.deb ...\n",
            "Unpacking libdecor-0-dev:amd64 (0.1.0-3build1) ...\n",
            "Selecting previously unselected package libpciaccess-dev:amd64.\n",
            "Preparing to unpack .../08-libpciaccess-dev_0.16-3_amd64.deb ...\n",
            "Unpacking libpciaccess-dev:amd64 (0.16-3) ...\n",
            "Selecting previously unselected package libdrm-dev:amd64.\n",
            "Preparing to unpack .../09-libdrm-dev_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libdrm-dev:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "Preparing to unpack .../10-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../11-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../12-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../13-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../14-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../15-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../16-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../17-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../18-libegl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libegl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libgbm-dev:amd64.\n",
            "Preparing to unpack .../19-libgbm-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libgbm-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../20-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglu1-mesa-dev:amd64.\n",
            "Preparing to unpack .../21-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libibus-1.0-dev:amd64.\n",
            "Preparing to unpack .../22-libibus-1.0-dev_1.5.26-4_amd64.deb ...\n",
            "Unpacking libibus-1.0-dev:amd64 (1.5.26-4) ...\n",
            "Selecting previously unselected package libpulse-mainloop-glib0:amd64.\n",
            "Preparing to unpack .../23-libpulse-mainloop-glib0_1%3a15.99.1+dfsg1-1ubuntu2.2_amd64.deb ...\n",
            "Unpacking libpulse-mainloop-glib0:amd64 (1:15.99.1+dfsg1-1ubuntu2.2) ...\n",
            "Selecting previously unselected package libpulse-dev:amd64.\n",
            "Preparing to unpack .../24-libpulse-dev_1%3a15.99.1+dfsg1-1ubuntu2.2_amd64.deb ...\n",
            "Unpacking libpulse-dev:amd64 (1:15.99.1+dfsg1-1ubuntu2.2) ...\n",
            "Selecting previously unselected package libsndio-dev:amd64.\n",
            "Preparing to unpack .../25-libsndio-dev_1.8.1-1.1_amd64.deb ...\n",
            "Unpacking libsndio-dev:amd64 (1.8.1-1.1) ...\n",
            "Selecting previously unselected package libudev-dev:amd64.\n",
            "Preparing to unpack .../26-libudev-dev_249.11-0ubuntu3.17_amd64.deb ...\n",
            "Unpacking libudev-dev:amd64 (249.11-0ubuntu3.17) ...\n",
            "Selecting previously unselected package libxfixes-dev:amd64.\n",
            "Preparing to unpack .../27-libxfixes-dev_1%3a6.0.0-1_amd64.deb ...\n",
            "Unpacking libxfixes-dev:amd64 (1:6.0.0-1) ...\n",
            "Selecting previously unselected package libxcursor-dev:amd64.\n",
            "Preparing to unpack .../28-libxcursor-dev_1%3a1.2.0-2build4_amd64.deb ...\n",
            "Unpacking libxcursor-dev:amd64 (1:1.2.0-2build4) ...\n",
            "Selecting previously unselected package libxi-dev:amd64.\n",
            "Preparing to unpack .../29-libxi-dev_2%3a1.8-1build1_amd64.deb ...\n",
            "Unpacking libxi-dev:amd64 (2:1.8-1build1) ...\n",
            "Selecting previously unselected package libxinerama-dev:amd64.\n",
            "Preparing to unpack .../30-libxinerama-dev_2%3a1.1.4-3_amd64.deb ...\n",
            "Unpacking libxinerama-dev:amd64 (2:1.1.4-3) ...\n",
            "Selecting previously unselected package libxkbcommon-dev:amd64.\n",
            "Preparing to unpack .../31-libxkbcommon-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libxrandr-dev:amd64.\n",
            "Preparing to unpack .../32-libxrandr-dev_2%3a1.5.2-1build1_amd64.deb ...\n",
            "Unpacking libxrandr-dev:amd64 (2:1.5.2-1build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../33-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package libxv-dev:amd64.\n",
            "Preparing to unpack .../34-libxv-dev_2%3a1.0.11-1build2_amd64.deb ...\n",
            "Unpacking libxv-dev:amd64 (2:1.0.11-1build2) ...\n",
            "Selecting previously unselected package libxxf86vm-dev:amd64.\n",
            "Preparing to unpack .../35-libxxf86vm-dev_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libxxf86vm-dev:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libsdl2-dev:amd64.\n",
            "Preparing to unpack .../36-libsdl2-dev_2.0.20+dfsg-2ubuntu1.22.04.1_amd64.deb ...\n",
            "Unpacking libsdl2-dev:amd64 (2.0.20+dfsg-2ubuntu1.22.04.1) ...\n",
            "Selecting previously unselected package python3-opengl.\n",
            "Preparing to unpack .../37-python3-opengl_3.1.5+dfsg-1_all.deb ...\n",
            "Unpacking python3-opengl (3.1.5+dfsg-1) ...\n",
            "Setting up libsndio-dev:amd64 (1.8.1-1.1) ...\n",
            "Setting up libpciaccess-dev:amd64 (0.16-3) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-6) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libxxf86vm-dev:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libxkbcommon-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libibus-1.0-5:amd64 (1.5.26-4) ...\n",
            "Setting up libgbm-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libxfixes-dev:amd64 (1:6.0.0-1) ...\n",
            "Setting up libxv-dev:amd64 (2:1.0.11-1build2) ...\n",
            "Setting up libwayland-bin (1.20.0-1ubuntu0.1) ...\n",
            "Setting up libxrandr-dev:amd64 (2:1.5.2-1build1) ...\n",
            "Setting up libdbus-1-dev:amd64 (1.12.20-2ubuntu4.1) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up libpulse-mainloop-glib0:amd64 (1:15.99.1+dfsg1-1ubuntu2.2) ...\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up libudev-dev:amd64 (249.11-0ubuntu3.17) ...\n",
            "Setting up libxinerama-dev:amd64 (2:1.1.4-3) ...\n",
            "Setting up libpulse-dev:amd64 (1:15.99.1+dfsg1-1ubuntu2.2) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libxi-dev:amd64 (2:1.8-1build1) ...\n",
            "Setting up libasound2-dev:amd64 (1.2.6.1-1ubuntu1) ...\n",
            "Setting up gir1.2-ibus-1.0:amd64 (1.5.26-4) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libdrm-dev:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libxcursor-dev:amd64 (1:1.2.0-2build4) ...\n",
            "Setting up python3-opengl (3.1.5+dfsg-1) ...\n",
            "Setting up libwayland-dev:amd64 (1.20.0-1ubuntu0.1) ...\n",
            "Setting up libibus-1.0-dev:amd64 (1.5.26-4) ...\n",
            "Setting up libdecor-0-dev:amd64 (0.1.0-3build1) ...\n",
            "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libegl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libsdl2-dev:amd64 (2.0.20+dfsg-2ubuntu1.22.04.1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Atari環境と録画に必要なシステム依存関係をインストール\n",
        "!apt-get update\n",
        "!apt-get install -y ffmpeg xvfb libsdl2-dev python3-opengl cmake zlib1g-dev"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghTUfsdE70oW"
      },
      "source": [
        "## 2. Pythonライブラリのインストール"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nREyrldI70oW",
        "outputId": "4240df80-9f23-467a-bd92-10d10565730b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (2.37.2)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.12/dist-packages (0.23.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: gymnasium[accept-rom-license,atari,classic_control] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,classic_control]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,classic_control]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,classic_control]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,classic_control]) (0.0.4)\n",
            "Requirement already satisfied: ale_py>=0.9 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,classic_control]) (0.11.2)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[accept-rom-license,atari,classic_control]) (2.6.1)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.32.4)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (3.1.45)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from wandb) (25.0)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb) (4.5.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb) (6.0.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb) (2.47.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install gymnasium[classic_control,atari,accept-rom-license] moviepy imageio wandb opencv-python matplotlib tqdm pillow torchvision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61uUAs_C70oX"
      },
      "source": [
        "## 3. 実行に必要なスクリプトファイルを作成\n",
        "作成されたファイルを用意できれば作成不要"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAU8G-Gq70oX",
        "outputId": "9c8d1fb3-faff-4c28-c71f-6ebd0a767d0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing student_code.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile student_code.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.distributions as td\n",
        "from torch.distributions import Normal, OneHotCategoricalStraightThrough\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "\n",
        "class MSE(td.Normal):\n",
        "    def __init__(self, loc, validate_args=None):\n",
        "        super(MSE, self).__init__(loc, 1.0, validate_args=validate_args)\n",
        "\n",
        "    @property\n",
        "    def mode(self):\n",
        "        return self.mean\n",
        "\n",
        "    def sample(self, sample_shape=torch.Size()):\n",
        "        return self.rsample(sample_shape)\n",
        "\n",
        "    def log_prob(self, value):\n",
        "        if self._validate_args:\n",
        "            self._validate_sample(value)\n",
        "        # NOTE: dropped the constant term\n",
        "        return -((value - self.loc) ** 2) / 2\n",
        "\n",
        "# From https://github.com/toshas/torch_truncnorm/blob/main/TruncatedNormal.py\n",
        "import math\n",
        "from numbers import Number\n",
        "\n",
        "import torch\n",
        "from torch.distributions import Distribution, constraints\n",
        "from torch.distributions.utils import broadcast_all\n",
        "\n",
        "CONST_SQRT_2 = math.sqrt(2)\n",
        "CONST_INV_SQRT_2PI = 1 / math.sqrt(2 * math.pi)\n",
        "CONST_INV_SQRT_2 = 1 / math.sqrt(2)\n",
        "CONST_LOG_INV_SQRT_2PI = math.log(CONST_INV_SQRT_2PI)\n",
        "CONST_LOG_SQRT_2PI_E = 0.5 * math.log(2 * math.pi * math.e)\n",
        "\n",
        "\n",
        "class TruncatedStandardNormal(Distribution):\n",
        "    \"\"\"\n",
        "    Truncated Standard Normal distribution\n",
        "    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    arg_constraints = {\n",
        "        'a': constraints.real,\n",
        "        'b': constraints.real,\n",
        "    }\n",
        "    has_rsample = True\n",
        "\n",
        "    def __init__(self, a, b, validate_args=None):\n",
        "        self.a, self.b = broadcast_all(a, b)\n",
        "        if isinstance(a, Number) and isinstance(b, Number):\n",
        "            batch_shape = torch.Size()\n",
        "        else:\n",
        "            batch_shape = self.a.size()\n",
        "        super(TruncatedStandardNormal, self).__init__(batch_shape, validate_args=validate_args)\n",
        "        if self.a.dtype != self.b.dtype:\n",
        "            raise ValueError('Truncation bounds types are different')\n",
        "        if any((self.a >= self.b).view(-1, ).tolist()):\n",
        "            raise ValueError('Incorrect truncation range')\n",
        "        eps = torch.finfo(self.a.dtype).eps\n",
        "        self._dtype_min_gt_0 = eps\n",
        "        self._dtype_max_lt_1 = 1 - eps\n",
        "        self._little_phi_a = self._little_phi(self.a)\n",
        "        self._little_phi_b = self._little_phi(self.b)\n",
        "        self._big_phi_a = self._big_phi(self.a)\n",
        "        self._big_phi_b = self._big_phi(self.b)\n",
        "        self._Z = (self._big_phi_b - self._big_phi_a).clamp_min(eps)\n",
        "        self._log_Z = self._Z.log()\n",
        "        little_phi_coeff_a = torch.nan_to_num(self.a, nan=math.nan)\n",
        "        little_phi_coeff_b = torch.nan_to_num(self.b, nan=math.nan)\n",
        "        self._lpbb_m_lpaa_d_Z = (self._little_phi_b * little_phi_coeff_b -\n",
        "                                 self._little_phi_a * little_phi_coeff_a) / self._Z\n",
        "        self._mean = -(self._little_phi_b - self._little_phi_a) / self._Z\n",
        "        # NOTE: additional to github.com/toshas/torch_truncnorm\n",
        "        self._mode = torch.clamp(torch.zeros_like(self.a), self.a, self.b)\n",
        "        self._variance = 1 - self._lpbb_m_lpaa_d_Z - ((self._little_phi_b - self._little_phi_a) / self._Z) ** 2\n",
        "        self._entropy = CONST_LOG_SQRT_2PI_E + self._log_Z - 0.5 * self._lpbb_m_lpaa_d_Z\n",
        "\n",
        "    @constraints.dependent_property\n",
        "    def support(self):\n",
        "        return constraints.interval(self.a, self.b)\n",
        "\n",
        "    @property\n",
        "    def mean(self):\n",
        "        return self._mean\n",
        "\n",
        "    @property\n",
        "    def mode(self):\n",
        "        return self._mode\n",
        "\n",
        "    @property\n",
        "    def variance(self):\n",
        "        return self._variance\n",
        "\n",
        "    def entropy(self):\n",
        "        return self._entropy\n",
        "\n",
        "    @property\n",
        "    def auc(self):\n",
        "        return self._Z\n",
        "\n",
        "    @staticmethod\n",
        "    def _little_phi(x):\n",
        "        return (-(x ** 2) * 0.5).exp() * CONST_INV_SQRT_2PI\n",
        "\n",
        "    @staticmethod\n",
        "    def _big_phi(x):\n",
        "        return 0.5 * (1 + (x * CONST_INV_SQRT_2).erf())\n",
        "\n",
        "    @staticmethod\n",
        "    def _inv_big_phi(x):\n",
        "        return CONST_SQRT_2 * (2 * x - 1).erfinv()\n",
        "\n",
        "    def cdf(self, value):\n",
        "        if self._validate_args:\n",
        "            self._validate_sample(value)\n",
        "        return ((self._big_phi(value) - self._big_phi_a) / self._Z).clamp(0, 1)\n",
        "\n",
        "    def icdf(self, value):\n",
        "        return self._inv_big_phi(self._big_phi_a + value * self._Z)\n",
        "\n",
        "    def log_prob(self, value):\n",
        "        if self._validate_args:\n",
        "            self._validate_sample(value)\n",
        "        return CONST_LOG_INV_SQRT_2PI - self._log_Z - (value ** 2) * 0.5\n",
        "\n",
        "    def rsample(self, sample_shape=torch.Size()):\n",
        "        # icdf is numerically unstable; as a consequence, so is rsample.\n",
        "        shape = self._extended_shape(sample_shape)\n",
        "        p = torch.empty(shape, device=self.a.device).uniform_(self._dtype_min_gt_0, self._dtype_max_lt_1)\n",
        "        return self.icdf(p)\n",
        "\n",
        "\n",
        "class TruncatedNormal(TruncatedStandardNormal):\n",
        "    \"\"\"\n",
        "    Truncated Normal distribution\n",
        "    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n",
        "    \"\"\"\n",
        "\n",
        "    has_rsample = True\n",
        "\n",
        "    def __init__(self, loc, scale, scalar_a, scalar_b, validate_args=None):\n",
        "        self.loc, self.scale, a, b = broadcast_all(loc, scale, scalar_a, scalar_b)\n",
        "        a = (a - self.loc) / self.scale\n",
        "        b = (b - self.loc) / self.scale\n",
        "        super(TruncatedNormal, self).__init__(a, b, validate_args=validate_args)\n",
        "        self._log_scale = self.scale.log()\n",
        "        self._mean = self._mean * self.scale + self.loc\n",
        "        self._mode = torch.clamp(self.loc, scalar_a, scalar_b)  # NOTE: additional to github.com/toshas/torch_truncnorm\n",
        "        self._variance = self._variance * self.scale ** 2\n",
        "        self._entropy += self._log_scale\n",
        "\n",
        "    def _to_std_rv(self, value):\n",
        "        return (value - self.loc) / self.scale\n",
        "\n",
        "    def _from_std_rv(self, value):\n",
        "        return value * self.scale + self.loc\n",
        "\n",
        "    def cdf(self, value):\n",
        "        return super(TruncatedNormal, self).cdf(self._to_std_rv(value))\n",
        "\n",
        "    def icdf(self, value):\n",
        "        return self._from_std_rv(super(TruncatedNormal, self).icdf(value))\n",
        "\n",
        "    def log_prob(self, value):\n",
        "        return super(TruncatedNormal, self).log_prob(self._to_std_rv(value)) - self._log_scale\n",
        "\n",
        "\n",
        "class TruncNormalDist(TruncatedNormal):\n",
        "\n",
        "    def __init__(self, loc, scale, low, high, clip=1e-6, mult=1):\n",
        "        super().__init__(loc, scale, low, high)\n",
        "        self._clip = clip\n",
        "        self._mult = mult\n",
        "\n",
        "        self.low = low\n",
        "        self.high = high\n",
        "\n",
        "    def sample(self, *args, **kwargs):\n",
        "        event = super().rsample(*args, **kwargs)\n",
        "        if self._clip:\n",
        "            clipped = torch.clamp(\n",
        "                event, self.low + self._clip, self.high - self._clip\n",
        "            )\n",
        "            event = event - event.detach() + clipped.detach()\n",
        "        if self._mult:\n",
        "            event *= self._mult\n",
        "        return event\n",
        "\n",
        "\n",
        "class RSSM(nn.Module):\n",
        "    def __init__(self, mlp_hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int, action_dim: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.rnn_hidden_dim = rnn_hidden_dim\n",
        "        self.state_dim = state_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.transition_hidden = nn.Linear(state_dim * num_classes + action_dim, mlp_hidden_dim)\n",
        "        self.transition = nn.GRUCell(mlp_hidden_dim, rnn_hidden_dim)\n",
        "\n",
        "        self.prior_hidden = nn.Linear(rnn_hidden_dim, mlp_hidden_dim)\n",
        "        self.prior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n",
        "\n",
        "        self.posterior_hidden = nn.Linear(rnn_hidden_dim + 1536, mlp_hidden_dim)\n",
        "        self.posterior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n",
        "\n",
        "    def recurrent(self, state: torch.Tensor, action: torch.Tensor, rnn_hidden: torch.Tensor):\n",
        "        hidden = F.elu(self.transition_hidden(torch.cat([state, action], dim=1)))\n",
        "        rnn_hidden = self.transition(hidden, rnn_hidden)\n",
        "\n",
        "        return rnn_hidden\n",
        "\n",
        "    def get_prior(self, rnn_hidden: torch.Tensor, detach=False):\n",
        "        hidden = F.elu(self.prior_hidden(rnn_hidden))\n",
        "        logits = self.prior_logits(hidden)\n",
        "        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n",
        "\n",
        "        prior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n",
        "        if detach:\n",
        "            detach_prior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n",
        "            return prior_dist, detach_prior\n",
        "        return prior_dist\n",
        "\n",
        "    def get_posterior(self, rnn_hidden: torch.Tensor, embedded_obs: torch.Tensor, detach=False):\n",
        "        hidden = F.elu(self.posterior_hidden(torch.cat([rnn_hidden, embedded_obs], dim=1)))\n",
        "        logits = self.posterior_logits(hidden)\n",
        "        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n",
        "\n",
        "        posterior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n",
        "        if detach:\n",
        "            detach_posterior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n",
        "            return posterior_dist, detach_posterior\n",
        "        return posterior_dist\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 48, kernel_size=4, stride=2)\n",
        "        self.conv2 = nn.Conv2d(48, 96, kernel_size=4, stride=2)\n",
        "        self.conv3 = nn.Conv2d(96, 192, kernel_size=4, stride=2)\n",
        "        self.conv4 = nn.Conv2d(192, 384, kernel_size=4, stride=2)\n",
        "\n",
        "    def forward(self, obs: torch.Tensor):\n",
        "        hidden = F.elu(self.conv1(obs))\n",
        "        hidden = F.elu(self.conv2(hidden))\n",
        "        hidden = F.elu(self.conv3(hidden))\n",
        "        embedded_obs = self.conv4(hidden).reshape(hidden.size(0), -1)\n",
        "\n",
        "        return embedded_obs\n",
        "\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(state_dim*num_classes + rnn_hidden_dim, 1536)\n",
        "        self.dc1 = nn.ConvTranspose2d(1536, 192, kernel_size=5, stride=2)\n",
        "        self.dc2 = nn.ConvTranspose2d(192, 96, kernel_size=5, stride=2)\n",
        "        self.dc3 = nn.ConvTranspose2d(96, 48, kernel_size=6, stride=2)\n",
        "        self.dc4 = nn.ConvTranspose2d(48, 3, kernel_size=6, stride=2)\n",
        "\n",
        "\n",
        "    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n",
        "        hidden = self.fc(torch.cat([state, rnn_hidden], dim=1))\n",
        "        hidden = hidden.view(hidden.size(0), 1536, 1, 1)\n",
        "        hidden = F.elu(self.dc1(hidden))\n",
        "        hidden = F.elu(self.dc2(hidden))\n",
        "        hidden = F.elu(self.dc3(hidden))\n",
        "        mean = self.dc4(hidden)\n",
        "\n",
        "        obs_dist = td.Independent(MSE(mean), 3)\n",
        "        return obs_dist\n",
        "\n",
        "\n",
        "class RewardModel(nn.Module):\n",
        "    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n",
        "        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = F.elu(self.fc2(hidden))\n",
        "        hidden = F.elu(self.fc3(hidden))\n",
        "        mean = self.fc4(hidden)\n",
        "\n",
        "        reward_dist = td.Independent(MSE(mean),  1)\n",
        "        return reward_dist\n",
        "\n",
        "\n",
        "class DiscountModel(nn.Module):\n",
        "    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n",
        "        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = F.elu(self.fc2(hidden))\n",
        "        hidden = F.elu(self.fc3(hidden))\n",
        "        mean= self.fc4(hidden)\n",
        "\n",
        "        discount_dist = td.Independent(td.Bernoulli(logits=mean),  1)\n",
        "        return discount_dist\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, action_dim: int, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.mean = nn.Linear(hidden_dim, action_dim)\n",
        "        self.std = nn.Linear(hidden_dim, action_dim)\n",
        "        self.min_stddev = 0.1\n",
        "        self.init_stddev = np.log(np.exp(5.0) - 1)\n",
        "\n",
        "    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor, eval: bool = False):\n",
        "        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = F.elu(self.fc2(hidden))\n",
        "        hidden = F.elu(self.fc3(hidden))\n",
        "        hidden = F.elu(self.fc4(hidden))\n",
        "        mean = self.mean(hidden)\n",
        "        stddev = self.std(hidden)\n",
        "\n",
        "        mean = torch.tanh(mean)\n",
        "        stddev = 2 * torch.sigmoid((stddev + self.init_stddev) / 2) + self.min_stddev\n",
        "        if eval:\n",
        "            action = mean\n",
        "            return action, None, None\n",
        "\n",
        "        action_dist = td.Independent(TruncNormalDist(mean, stddev, -1, 1), 1)\n",
        "        action = action_dist.sample()\n",
        "\n",
        "        action_log_prob = action_dist.log_prob(action)\n",
        "        action_entropy = action_dist.entropy()\n",
        "\n",
        "        return action, action_log_prob, action_entropy\n",
        "\n",
        "\n",
        "class DiscreteActor(nn.Module):\n",
        "    def __init__(self, action_dim: int, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor, eval: bool = False):\n",
        "        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = F.elu(self.fc2(hidden))\n",
        "        hidden = F.elu(self.fc3(hidden))\n",
        "        hidden = F.elu(self.fc4(hidden))\n",
        "        logits = self.out(hidden)\n",
        "\n",
        "        if eval:\n",
        "            action = torch.argmax(logits, dim=-1)\n",
        "            return action, None, None\n",
        "\n",
        "        action_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n",
        "        action = action_dist.sample()\n",
        "\n",
        "        action_log_prob = action_dist.log_prob(action)\n",
        "        action_entropy = action_dist.entropy()\n",
        "\n",
        "        return action, action_log_prob, action_entropy\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor):\n",
        "        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n",
        "        hidden = F.elu(self.fc2(hidden))\n",
        "        hidden = F.elu(self.fc3(hidden))\n",
        "        hidden = F.elu(self.fc4(hidden))\n",
        "        mean = self.out(hidden)\n",
        "\n",
        "        return mean\n",
        "\n",
        "\n",
        "class ErrorPredictor(nn.Module):\n",
        "    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int, action_dim: int):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim + action_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.fc4 = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor, action: torch.Tensor):\n",
        "        x = torch.cat([state, rnn_hidden, action], dim=1)\n",
        "\n",
        "        hidden = F.elu(self.fc1(x))\n",
        "        hidden = F.elu(self.fc2(hidden))\n",
        "        hidden = F.elu(self.fc3(hidden))\n",
        "\n",
        "        pred_error = self.fc4(hidden)\n",
        "        return pred_error\n",
        "\n",
        "\n",
        "class Agent(nn.Module):\n",
        "    def __init__(self, encoder, decoder, rssm, action_model, error_predictor=None):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.rssm = rssm\n",
        "        self.action_model = action_model\n",
        "        self.error_predictor = error_predictor\n",
        "\n",
        "        self.device = next(self.action_model.parameters()).device\n",
        "        self.rnn_hidden = torch.zeros(1, rssm.rnn_hidden_dim, device=self.device)\n",
        "\n",
        "    def __call__(self, obs, eval=True):\n",
        "        obs = preprocess_obs(obs)\n",
        "        obs = torch.as_tensor(obs, device=self.device)\n",
        "        obs = obs.transpose(1, 2).transpose(0, 1).unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            state_prior = self.rssm.get_prior(self.rnn_hidden)\n",
        "            state_pred = state_prior.sample().flatten(1)\n",
        "            obs_dist = self.decoder(state_pred, self.rnn_hidden)\n",
        "            obs_pred_img = obs_dist.mean\n",
        "\n",
        "            embedded_obs = self.encoder(obs)\n",
        "            state_posterior = self.rssm.get_posterior(self.rnn_hidden, embedded_obs)\n",
        "            state = state_posterior.sample().flatten(1)\n",
        "\n",
        "            action, _, _  = self.action_model(state, self.rnn_hidden, eval=eval)\n",
        "\n",
        "            self.last_state = state\n",
        "            self.last_rnn_hidden = self.rnn_hidden\n",
        "            if state.ndim == 1:\n",
        "                state = state.unsqueeze(0)\n",
        "\n",
        "            if action.ndim == 1:\n",
        "                action = action.unsqueeze(0)\n",
        "\n",
        "            if action.shape[-1] == 1 and self.rssm.transition_hidden.in_features > (state.shape[-1] + 1):\n",
        "                 num_classes = self.rssm.transition_hidden.in_features - state.shape[-1]\n",
        "                 action_idx = action.long()\n",
        "                 action = F.one_hot(action_idx, num_classes=num_classes).float().squeeze(1)\n",
        "\n",
        "            self.rnn_hidden = self.rssm.recurrent(state, action, self.rnn_hidden)\n",
        "\n",
        "        return action.squeeze().cpu().numpy(), (obs_pred_img.squeeze().cpu().numpy().transpose(1, 2, 0) + 0.5).clip(0.0, 1.0)\n",
        "\n",
        "    def reset(self):\n",
        "        self.rnn_hidden = torch.zeros(1, self.rssm.rnn_hidden_dim, device=self.device)\n",
        "        self.last_state = None\n",
        "        self.last_rnn_hidden = None\n",
        "\n",
        "    def to(self, device):\n",
        "        self.device = device\n",
        "        self.encoder.to(device)\n",
        "        self.decoder.to(device)\n",
        "        self.rssm.to(device)\n",
        "        if self.error_predictor:\n",
        "            self.error_predictor.to(device)\n",
        "        self.rnn_hidden = self.rnn_hidden.to(device)\n",
        "        return self\n",
        "\n",
        "\n",
        "def preprocess_obs(obs):\n",
        "    obs = obs.astype(np.float32)\n",
        "    normalized_obs = obs / 255.0 - 0.5\n",
        "    return normalized_obs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FzJzDD570oa",
        "outputId": "392ccd5c-63ce-4b21-d9a6-5b913d24c082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting train_intrinsic.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile train_intrinsic.py\n",
        "import sys\n",
        "import atexit\n",
        "import os\n",
        "import random\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Normal, OneHotCategoricalStraightThrough\n",
        "from torch.distributions.kl import kl_divergence\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Import from student_code and exploration\n",
        "from student_code import Agent, RSSM, Encoder, Decoder, RewardModel, DiscountModel, Actor, DiscreteActor, Critic, MSE\n",
        "import gymnasium as gym\n",
        "import imageio\n",
        "try:\n",
        "    import craftium\n",
        "except ImportError:\n",
        "    craftium = None\n",
        "from PIL import Image\n",
        "import wandb\n",
        "from torchvision import datasets, transforms\n",
        "import random\n",
        "import os\n",
        "\n",
        "# Try importing stable_baselines3 wrappers if available, else define minimal ones\n",
        "try:\n",
        "    from stable_baselines3.common.atari_wrappers import (\n",
        "        NoopResetEnv, MaxAndSkipEnv, WarpFrame, ClipRewardEnv\n",
        "    )\n",
        "    SB3_AVAILABLE = True\n",
        "except ImportError:\n",
        "    SB3_AVAILABLE = False\n",
        "\n",
        "class MnistEnv(gym.Env):\n",
        "    def __init__(self, render_mode=None):\n",
        "        self.render_mode = render_mode\n",
        "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(64, 64, 3), dtype=np.uint8)\n",
        "        self.action_space = gym.spaces.Discrete(10) # Dummy action space\n",
        "\n",
        "        # Load MNIST data\n",
        "        try:\n",
        "            self.mnist_data = datasets.MNIST('../data', train=True, download=True,\n",
        "                                           transform=transforms.Compose([\n",
        "                                               transforms.Resize((64, 64)),\n",
        "                                               transforms.ToTensor()\n",
        "                                           ]))\n",
        "        except:\n",
        "            # Fallback if download fails or internet issue, generate random noise or try local\n",
        "            print(\"Warning: Could not load actual MNIST, using random noise placeholder for MnistEnv\")\n",
        "            self.mnist_data = None\n",
        "\n",
        "        self.current_idx = 0\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        self.current_idx = random.randint(0, len(self.mnist_data) - 1) if self.mnist_data else 0\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        # Action changes the image randomly to simulate \"watching\" different channels/digits\n",
        "        self.current_idx = random.randint(0, len(self.mnist_data) - 1) if self.mnist_data else 0\n",
        "        obs = self._get_obs()\n",
        "        reward = 0.0 # No extrinsic reward for just watching\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        return obs, reward, terminated, truncated, {}\n",
        "\n",
        "    def _get_obs(self):\n",
        "        if self.mnist_data:\n",
        "            img, _ = self.mnist_data[self.current_idx]\n",
        "            # Convert tensor (C, H, W) 0-1 to numpy (H, W, C) 0-255\n",
        "            img = img.permute(1, 2, 0).numpy() * 255.0\n",
        "            img = img.astype(np.uint8)\n",
        "            # MNIST is grayscale (1 channel), convert to RGB (3 channels) for consistency\n",
        "            if img.shape[2] == 1:\n",
        "                img = np.concatenate([img]*3, axis=2)\n",
        "            return img\n",
        "        else:\n",
        "            return np.random.randint(0, 255, (64, 64, 3), dtype=np.uint8)\n",
        "\n",
        "    def render(self):\n",
        "        return self._get_obs()\n",
        "\n",
        "# Config Class from Notebook\n",
        "class Config:\n",
        "    def __init__(self, **kwargs):\n",
        "        # data settings\n",
        "        self.buffer_size = 100_000\n",
        "        self.batch_size = 16\n",
        "        self.seq_length = 50\n",
        "        self.imagination_horizon = 20\n",
        "\n",
        "        # model dimensions\n",
        "        self.state_dim = 32\n",
        "        self.num_classes = 32\n",
        "        self.rnn_hidden_dim = 400\n",
        "        self.mlp_hidden_dim = 300\n",
        "\n",
        "        # learning params\n",
        "        self.model_lr = 2e-4\n",
        "        self.actor_lr = 4e-5\n",
        "        self.critic_lr = 1e-4\n",
        "        self.epsilon = 1e-5\n",
        "        self.weight_decay = 1e-6\n",
        "        self.gradient_clipping = 100\n",
        "        self.kl_scale = 0.1\n",
        "        self.kl_balance = 0.8\n",
        "        self.actor_entropy_scale = 1e-3\n",
        "        self.slow_critic_update = 100\n",
        "        self.reward_loss_scale = 1.0\n",
        "        self.discount_loss_scale = 1.0\n",
        "        self.update_freq = 80\n",
        "\n",
        "        # lambda return params\n",
        "        self.discount = 0.995\n",
        "        self.lambda_ = 0.95\n",
        "\n",
        "        # learning period settings\n",
        "        self.iter = 6000\n",
        "        self.seed_iter = 1000 # Reduced for faster start in dev\n",
        "        self.eval_interval = 10000 # 150000 画像保存のため変更\n",
        "        self.eval_freq = 5\n",
        "        self.eval_episodes = 5\n",
        "\n",
        "        # Intrinsic Reward Params\n",
        "        self.intr_reward_scale = 1.0\n",
        "\n",
        "        # Override with kwargs\n",
        "        for k, v in kwargs.items():\n",
        "            setattr(self, k, v)\n",
        "\n",
        "\n",
        "# Replay Buffer\n",
        "class ReplayBuffer(object):\n",
        "    def __init__(self, capacity, observation_shape, action_dim):\n",
        "        self.capacity = capacity\n",
        "        self.observations = np.zeros((capacity, *observation_shape), dtype=np.float32)\n",
        "        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n",
        "        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n",
        "        self.done = np.zeros((capacity, 1), dtype=bool)\n",
        "        self.index = 0\n",
        "        self.is_filled = False\n",
        "\n",
        "    def push(self, observation, action, reward, done):\n",
        "        self.observations[self.index] = observation\n",
        "        self.actions[self.index] = action\n",
        "        self.rewards[self.index] = reward\n",
        "        self.done[self.index] = done\n",
        "        if self.index == self.capacity - 1:\n",
        "            self.is_filled = True\n",
        "        self.index = (self.index + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size, chunk_length):\n",
        "        episode_borders = np.where(self.done)[0]\n",
        "        sampled_indexes = []\n",
        "        for _ in range(batch_size):\n",
        "            cross_border = True\n",
        "            while cross_border:\n",
        "                initial_index = np.random.randint(len(self) - chunk_length + 1)\n",
        "                final_index = initial_index + chunk_length - 1\n",
        "                cross_border = np.logical_and(initial_index <= episode_borders,\n",
        "                                              episode_borders < final_index).any()\n",
        "            sampled_indexes += list(range(initial_index, final_index + 1))\n",
        "\n",
        "        sampled_observations = self.observations[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, *self.observations.shape[1:])\n",
        "        sampled_actions = self.actions[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, self.actions.shape[1])\n",
        "        sampled_rewards = self.rewards[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, 1)\n",
        "        sampled_done = self.done[sampled_indexes].reshape(\n",
        "            batch_size, chunk_length, 1)\n",
        "        return sampled_observations, sampled_actions, sampled_rewards, sampled_done\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.capacity if self.is_filled else self.index\n",
        "\n",
        "def preprocess_obs(obs):\n",
        "    obs = obs.astype(np.float32)\n",
        "    normalized_obs = obs / 255.0 - 0.5\n",
        "    return normalized_obs\n",
        "\n",
        "def calculate_lambda_target(rewards, discounts, values, lambda_):\n",
        "    V_lambda = torch.zeros_like(rewards)\n",
        "    for t in reversed(range(rewards.shape[0])):\n",
        "        if t == rewards.shape[0] - 1:\n",
        "            V_lambda[t] = rewards[t] + discounts[t] * values[t]\n",
        "        else:\n",
        "            V_lambda[t] = rewards[t] + discounts[t] * ((1-lambda_) * values[t+1] + lambda_ * V_lambda[t+1])\n",
        "    return V_lambda\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def evaluation(eval_env, policy, step, cfg):\n",
        "    env = eval_env\n",
        "    all_ep_rewards = []\n",
        "\n",
        "    # Ensure directories exist\n",
        "    os.makedirs(\"eval_view/video\", exist_ok=True)\n",
        "    os.makedirs(\"eval_view/images\", exist_ok=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(cfg.eval_episodes):\n",
        "            obs = env.reset()\n",
        "            if isinstance(obs, tuple): obs = obs[0] # Gym API handling\n",
        "            policy.reset()\n",
        "            done = False\n",
        "            truncated = False\n",
        "            episode_reward = []\n",
        "            frames = []\n",
        "            recon_frames = []\n",
        "\n",
        "            while not done and not truncated:\n",
        "                # Agent returns (action, recon_img)\n",
        "                # recon_img is (64, 64, 3) range 0-1\n",
        "                action, recon_img = policy(obs, eval=True)\n",
        "\n",
        "                # Convert action to one-hot index if needed, but here simple env\n",
        "                if isinstance(env.action_space, gym.spaces.Box):\n",
        "                    # Clip action to space?\n",
        "                    # policy returns numpy array\n",
        "                    action = np.clip(action, env.action_space.low, env.action_space.high)\n",
        "                    obs, reward, done, truncated, info = env.step(action)\n",
        "                elif hasattr(env.action_space, 'n'):\n",
        "                    action_scalar = np.argmax(action)\n",
        "                    obs, reward, done, truncated, info = env.step(action_scalar)\n",
        "                else:\n",
        "                    obs, reward, done, truncated, info = env.step(action)\n",
        "\n",
        "                # Render for video (Real observation)\n",
        "                frame = env.render()\n",
        "                frames.append(frame)\n",
        "\n",
        "                # Reconstructed frame\n",
        "                if recon_img is not None:\n",
        "                    # recon_img is 0-1 float. Convert to 0-255 uint8\n",
        "                    recon_frame = (recon_img * 255.0).astype(np.uint8)\n",
        "                    recon_frames.append(recon_frame)\n",
        "\n",
        "                episode_reward.append(reward)\n",
        "\n",
        "            # Save video and images for the first episode of evaluation\n",
        "            if i == 0 and len(frames) > 0:\n",
        "                # Save Video (Side by Side if possible, or separate)\n",
        "                video_path = f\"eval_view/video/eval_iter_{step}_ep_{i}.mp4\"\n",
        "                try:\n",
        "                     # Ensure frames are uint8 numpy arrays\n",
        "                     frames = [np.array(f, dtype=np.uint8) for f in frames if f is not None]\n",
        "\n",
        "                     if len(recon_frames) > 0 and len(recon_frames) == len(frames):\n",
        "                         # Create side-by-side video\n",
        "                         # Resize frames to match if needed (env.render might be larger than 64x64)\n",
        "                         # recon is 64x64. frame depends on env.\n",
        "\n",
        "                         combined_frames = []\n",
        "                         for f, r in zip(frames, recon_frames):\n",
        "                             # Resize f to match r (64x64) for simple concatenation\n",
        "                             f_pil = Image.fromarray(f)\n",
        "                             r_pil = Image.fromarray(r)\n",
        "\n",
        "                             f_s = f_pil.resize((64, 64))\n",
        "\n",
        "                             # Concatenate horizontally\n",
        "                             comb = Image.new('RGB', (128, 64))\n",
        "                             comb.paste(f_s, (0, 0))\n",
        "                             comb.paste(r_pil, (64, 0))\n",
        "                             combined_frames.append(np.array(comb))\n",
        "\n",
        "                         imageio.mimsave(video_path, combined_frames, fps=30)\n",
        "                         print(f\"Saved combined video to {video_path}\")\n",
        "                     else:\n",
        "                        imageio.mimsave(video_path, frames, fps=30)\n",
        "                        print(f\"Saved video to {video_path}\")\n",
        "\n",
        "                     # Save Snapshot Images (Start, Middle, End)\n",
        "                     if len(combined_frames) > 0:\n",
        "                         indices = [0, len(combined_frames)//2, len(combined_frames)-1]\n",
        "                         for idx in indices:\n",
        "                             img_path = f\"eval_view/images/eval_iter_{step}_ep_{i}_frame_{idx}.png\"\n",
        "                             Image.fromarray(combined_frames[idx]).save(img_path)\n",
        "\n",
        "                except Exception as e:\n",
        "                     print(f\"Failed to save video: {e}\")\n",
        "\n",
        "            if len(episode_reward) < 500:\n",
        "                pad_val = 10 if info.get(\"success\", False) else 0\n",
        "                episode_reward = np.pad(episode_reward, (0, 500 - len(episode_reward)), \"constant\", constant_values=pad_val)\n",
        "\n",
        "            all_ep_rewards.append(np.mean(episode_reward))\n",
        "\n",
        "        mean_ep_rewards = np.mean(all_ep_rewards)\n",
        "        max_ep_rewards = np.max(all_ep_rewards)\n",
        "        print(f\"Eval(iter={step}) mean: {mean_ep_rewards:.4f} max: {max_ep_rewards:.4f}\")\n",
        "\n",
        "    return mean_ep_rewards\n",
        "\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--env-name', default='Pendulum-v1') # Changed default to current working one, or keep generic\n",
        "    # parser.add_argument('--noisy-tv', action='store_true') #復活あるかも、でもコメント消しても機能はしない。\n",
        "    parser.add_argument('--seed', type=int, default=1)\n",
        "    parser.add_argument('--steps', type=int, default=200000)\n",
        "    # WandB arguments\n",
        "    parser.add_argument('--wandb', action='store_true', help='Use WandB logging')\n",
        "    parser.add_argument('--wandb-project', default='Dreamer-LPM', help='WandB project name')\n",
        "    parser.add_argument('--wandb-entity', default=None, help='WandB entity')\n",
        "    parser.add_argument('--wandb-run-name', default=None, help='Run name')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    cfg = Config()\n",
        "    cfg.iter = args.steps\n",
        "\n",
        "    # Initialize WandB\n",
        "    if args.wandb:\n",
        "        wandb.init(\n",
        "            project=args.wandb_project,\n",
        "            entity=args.wandb_entity,\n",
        "            name=args.wandb_run_name,\n",
        "            config=vars(cfg),\n",
        "            mode=\"online\"\n",
        "        )\n",
        "\n",
        "    set_seed(args.seed)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Simple env factory\n",
        "    def make_env_simple(seed, env_name, noisy=False):\n",
        "        env = None\n",
        "        trigger_thresh = None\n",
        "        trigger_prob = 0.01\n",
        "\n",
        "        if env_name == \"Craftium\":\n",
        "            env = gym.make(\"Craftium/OpenWorld-v0\", render_mode=\"rgb_array\")\n",
        "        elif env_name.startswith(\"Craftium/\"):\n",
        "            env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "        elif env_name == \"MountainCarContinuous-v0\":\n",
        "             env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"rgb_array\")\n",
        "             trigger_thresh = 0.5\n",
        "        elif env_name == \"Pendulum-v1\":\n",
        "             env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
        "             trigger_thresh = 1.0\n",
        "             trigger_prob = 0.05\n",
        "        elif env_name == \"MNIST\":\n",
        "            env = MnistEnv()\n",
        "            trigger_prob = 0.1\n",
        "        elif \"ALE/\" in env_name or \"NoFrameskip\" in env_name or \"Breakout\" in env_name:\n",
        "            import ale_py\n",
        "            gym.register_envs(ale_py) # Just in case, or just importing is enough usually\n",
        "\n",
        "            env = gym.make(env_name, render_mode=\"rgb_array\", frameskip=1) # frameskip handled by wrapper\n",
        "\n",
        "            from gymnasium.wrappers import AtariPreprocessing, TransformReward\n",
        "\n",
        "            env = AtariPreprocessing(env, noop_max=30, frame_skip=4, screen_size=64, terminal_on_life_loss=False, grayscale_obs=False, scale_obs=False)\n",
        "\n",
        "            env = TransformReward(env, lambda r: np.sign(r))\n",
        "\n",
        "            trigger_prob = 0.01\n",
        "        else:\n",
        "             try:\n",
        "                env = gym.make(env_name, render_mode=\"rgb_array\")\n",
        "             except:\n",
        "                print(f\"Could not make environment {env_name}, defaulting to Pendulum\")\n",
        "                env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
        "\n",
        "        if not isinstance(env, MnistEnv):\n",
        "            from gymnasium.wrappers import ResizeObservation\n",
        "            env = ResizeObservation(env, (64, 64))\n",
        "\n",
        "        return env\n",
        "\n",
        "    env = make_env_simple(args.seed, args.env_name, args.noisy_tv)\n",
        "    atexit.register(env.close)\n",
        "\n",
        "    eval_env = make_env_simple(args.seed + 100, args.env_name, args.noisy_tv)\n",
        "    atexit.register(eval_env.close)\n",
        "    # 将来的に動画保存用\n",
        "    # if not os.path.exists(\"videos\"):\n",
        "    #    os.makedirs(\"videos\")\n",
        "\n",
        "    if isinstance(env.action_space, gym.spaces.Box):\n",
        "        action_dim = env.action_space.shape[0]\n",
        "    else:\n",
        "        action_dim = env.action_space.n\n",
        "\n",
        "    replay_buffer = ReplayBuffer(\n",
        "        capacity=cfg.buffer_size,\n",
        "        observation_shape=(64, 64, 3), # Assuming 3 channels\n",
        "        action_dim=action_dim\n",
        "    )\n",
        "\n",
        "    rssm = RSSM(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes, action_dim).to(device)\n",
        "    encoder = Encoder().to(device)\n",
        "    decoder = Decoder(cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "    reward_model = RewardModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "         actor = DiscreteActor(action_dim, cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "    else:\n",
        "         actor = Actor(action_dim, cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "    critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "    target_critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n",
        "    target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "    agent = Agent(encoder, decoder, rssm, actor).to(device)\n",
        "\n",
        "    wm_params = list(rssm.parameters()) + list(encoder.parameters()) + list(decoder.parameters()) + list(reward_model.parameters())\n",
        "    wm_optimizer = torch.optim.Adam(wm_params, lr=cfg.model_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n",
        "    actor_optimizer = torch.optim.Adam(actor.parameters(), lr=cfg.actor_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n",
        "    critic_optimizer = torch.optim.Adam(critic.parameters(), lr=cfg.critic_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n",
        "\n",
        "    obs = env.reset()\n",
        "    if isinstance(obs, tuple): obs = obs[0]\n",
        "\n",
        "    total_episode = 1\n",
        "    total_reward = []\n",
        "\n",
        "    print(\"Pre-filling buffer...\")\n",
        "    for _ in range(cfg.seed_iter):\n",
        "        action = env.action_space.sample()\n",
        "        next_obs, reward, done, truncated, _ = env.step(action)\n",
        "        done_flag = done or truncated\n",
        "\n",
        "        replay_buffer.push(preprocess_obs(obs), action, reward, done_flag)\n",
        "        obs = next_obs\n",
        "        if done_flag:\n",
        "            obs = env.reset()\n",
        "            if isinstance(obs, tuple): obs = obs[0]\n",
        "\n",
        "    lpm_log_file = \"intrinsic_stats.csv\"\n",
        "    with open(lpm_log_file, \"w\") as f:\n",
        "        f.write(\"step,actual_error,intr_reward,is_noisy\\n\")\n",
        "\n",
        "    print(\"Starting Main Loop...\")\n",
        "\n",
        "    pbar = tqdm(range(cfg.iter), desc=\"Training Steps\", unit=\"step\")\n",
        "    for iteration in pbar:\n",
        "\n",
        "        with torch.no_grad():\n",
        "            action, pred_obs_normalized = agent(obs, eval=False)\n",
        "\n",
        "            if isinstance(env.action_space, gym.spaces.Discrete):\n",
        "                 if action.ndim > 1:\n",
        "                      action_scalar = np.argmax(action, axis=1)[0]\n",
        "                 else:\n",
        "                      action_scalar = np.argmax(action)\n",
        "                 env_action = action_scalar\n",
        "            elif isinstance(env.action_space, gym.spaces.Box):\n",
        "                 if action.ndim == 0:\n",
        "                     action = np.expand_dims(action, axis=0)\n",
        "                 env_action = action\n",
        "\n",
        "            next_obs, reward, done, truncated, info = env.step(env_action)\n",
        "            done_flag = done or truncated\n",
        "\n",
        "            next_obs_normalized = preprocess_obs(next_obs)\n",
        "            t_last_state = agent.last_state\n",
        "            t_last_rnn = agent.last_rnn_hidden\n",
        "\n",
        "            if t_last_state is not None:\n",
        "                t_action = torch.as_tensor(action, device=device).unsqueeze(0)\n",
        "\n",
        "                t_next_rnn = rssm.recurrent(t_last_state, t_action, t_last_rnn)\n",
        "                t_next_prior = rssm.get_prior(t_next_rnn)\n",
        "                t_next_state = t_next_prior.mean.flatten(1)\n",
        "\n",
        "                t_next_obs_dist = decoder(t_next_state, t_next_rnn)\n",
        "                t_next_obs_pred = t_next_obs_dist.mean.squeeze().cpu().numpy()\n",
        "                t_next_obs_pred = t_next_obs_pred.transpose(1, 2, 0)\n",
        "\n",
        "                actual_error = np.mean((next_obs_normalized - t_next_obs_pred)**2)\n",
        "\n",
        "                actual_error_log = np.log(actual_error + 1e-6)\n",
        "                intr_reward = cfg.intr_reward_scale * actual_error_log\n",
        "\n",
        "                total_reward_val = reward + intr_reward\n",
        "\n",
        "                is_noisy_step = info.get(\"noisy\", False)\n",
        "                with open(lpm_log_file, \"a\") as f:\n",
        "                    f.write(f\"{iteration},{actual_error},{intr_reward},{is_noisy_step}\\n\")\n",
        "\n",
        "                if args.wandb:\n",
        "                    metrics = {\n",
        "                        \"Intrinsic/Actual_Error\": actual_error,\n",
        "                        \"Intrinsic/Intrinsic_Reward\": intr_reward,\n",
        "                    }\n",
        "                    if is_noisy_step:\n",
        "                        metrics[\"Intrinsic/Intrinsic_Reward_Noisy\"] = intr_reward\n",
        "                        metrics[\"Intrinsic/Actual_Error_Noisy\"] = actual_error\n",
        "                    else:\n",
        "                        metrics[\"Intrinsic/Intrinsic_Reward_Clean\"] = intr_reward\n",
        "                        metrics[\"Intrinsic/Actual_Error_Clean\"] = actual_error\n",
        "\n",
        "                    wandb.log(metrics, step=iteration)\n",
        "            else:\n",
        "                total_reward_val = reward\n",
        "\n",
        "            replay_buffer.push(preprocess_obs(obs), action, total_reward_val, done_flag)\n",
        "\n",
        "            obs = next_obs\n",
        "            total_reward.append(reward)\n",
        "\n",
        "            if done_flag:\n",
        "                obs = env.reset()\n",
        "                if isinstance(obs, tuple): obs = obs[0]\n",
        "                agent.reset()\n",
        "\n",
        "                print(f\"Episode {total_episode} ExtReward: {np.mean(total_reward):.4f}\")\n",
        "\n",
        "                if args.wandb:\n",
        "                    wandb.log({\n",
        "                        \"Episode/Extrinsic_Reward\": np.mean(total_reward),\n",
        "                        \"Episode/Steps\": iteration,\n",
        "                        \"Episode/ID\": total_episode\n",
        "                    })\n",
        "\n",
        "                total_reward = []\n",
        "                total_episode += 1\n",
        "\n",
        "        if (iteration + 1) % cfg.eval_interval == 0:\n",
        "            try:\n",
        "                evaluation(eval_env, agent, iteration, cfg)\n",
        "                eval_env.reset()\n",
        "                agent.reset()\n",
        "            except Exception as e:\n",
        "                print(f\"Evaluation failed at iteration {iteration}: {e}\")\n",
        "                print(\"Re-creating evaluation environment...\")\n",
        "                try:\n",
        "                    eval_env.close()\n",
        "                except:\n",
        "                    pass\n",
        "                try:\n",
        "                    eval_env = make_env_simple(args.seed + 100 + iteration, args.env_name, args.noisy_tv)\n",
        "                    print(\"Evaluation environment re-created successfully.\")\n",
        "                except Exception as create_e:\n",
        "                    print(f\"Failed to re-create evaluation environment: {create_e}\")\n",
        "\n",
        "        if (iteration + 1) % cfg.update_freq == 0:\n",
        "            observations, actions, rewards, done_flags = replay_buffer.sample(cfg.batch_size, cfg.seq_length)\n",
        "            done_flags = 1 - done_flags\n",
        "\n",
        "            observations = torch.permute(torch.as_tensor(observations, device=device), (1, 0, 4, 2, 3))\n",
        "            actions = torch.as_tensor(actions, device=device).transpose(0, 1)\n",
        "            rewards = torch.as_tensor(rewards, device=device).transpose(0, 1)\n",
        "            done_flags = torch.as_tensor(done_flags, device=device).transpose(0, 1).float()\n",
        "\n",
        "            emb_observations = encoder(observations.reshape(-1, 3, 64, 64)).view(cfg.seq_length, cfg.batch_size, -1)\n",
        "\n",
        "            state = torch.zeros(cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n",
        "            rnn_hidden = torch.zeros(cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n",
        "\n",
        "            states = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n",
        "            rnn_hiddens = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n",
        "\n",
        "            kl_loss = 0\n",
        "\n",
        "            for i in range(cfg.seq_length-1):\n",
        "                rnn_hidden = rssm.recurrent(state, actions[i], rnn_hidden)\n",
        "\n",
        "                next_state_prior, next_detach_prior = rssm.get_prior(rnn_hidden, detach=True)\n",
        "                next_state_posterior, next_detach_posterior = rssm.get_posterior(rnn_hidden, emb_observations[i+1], detach=True)\n",
        "\n",
        "                state = next_state_posterior.rsample().flatten(1)\n",
        "                rnn_hiddens[i+1] = rnn_hidden\n",
        "                states[i+1] = state\n",
        "\n",
        "                kl_loss += cfg.kl_balance * torch.mean(kl_divergence(next_detach_posterior, next_state_prior))\n",
        "                (1 - cfg.kl_balance) * torch.mean(kl_divergence(next_state_posterior, next_detach_prior))\n",
        "\n",
        "            kl_loss /= (cfg.seq_length - 1)\n",
        "\n",
        "            rnn_hiddens = rnn_hiddens[1:]\n",
        "            states = states[1:]\n",
        "\n",
        "            flatten_rnn_hiddens = rnn_hiddens.view(-1, cfg.rnn_hidden_dim)\n",
        "            flatten_states = states.view(-1, cfg.state_dim * cfg.num_classes)\n",
        "\n",
        "            obs_dist = decoder(flatten_states, flatten_rnn_hiddens)\n",
        "            reward_dist = reward_model(flatten_states, flatten_rnn_hiddens)\n",
        "\n",
        "            C, H, W = observations.shape[2:]\n",
        "            obs_loss = -torch.mean(obs_dist.log_prob(observations[1:].reshape(-1, C, H, W)))\n",
        "            reward_loss = -torch.mean(reward_dist.log_prob(rewards[:-1].reshape(-1, 1)))\n",
        "\n",
        "            wm_loss = obs_loss + cfg.reward_loss_scale * reward_loss + cfg.kl_scale * kl_loss\n",
        "\n",
        "            wm_optimizer.zero_grad()\n",
        "            wm_loss.backward()\n",
        "            clip_grad_norm_(wm_params, cfg.gradient_clipping)\n",
        "            wm_optimizer.step()\n",
        "\n",
        "            if args.wandb:\n",
        "                wandb.log({\n",
        "                    \"Train/WM_Loss\": wm_loss.item(),\n",
        "                    \"Train/Obs_Loss\": obs_loss.item(),\n",
        "                    \"Train/Reward_Loss\": reward_loss.item(),\n",
        "                    \"Train/KL_Loss\": kl_loss.item(),\n",
        "                    \"Train/Reward_Mean\": rewards.mean().item(),\n",
        "                }, step=iteration)\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                 \"total_r\": f\"{np.mean(total_reward[-10:]):.1f}\" if total_reward else \"0.0\",\n",
        "                 \"wm_loss\": f\"{wm_loss.item():.2f}\"\n",
        "            })\n",
        "\n",
        "            flatten_rnn_hiddens = flatten_rnn_hiddens.detach()\n",
        "            flatten_states = flatten_states.detach()\n",
        "\n",
        "            imagined_states = torch.zeros(cfg.imagination_horizon + 1, *flatten_states.shape, device=device)\n",
        "            imagined_rnn_hiddens = torch.zeros(cfg.imagination_horizon + 1, *flatten_rnn_hiddens.shape, device=device)\n",
        "            imagined_action_log_probs = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)), device=device)\n",
        "            imagined_action_entropys = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)), device=device)\n",
        "\n",
        "            imagined_states[0] = flatten_states\n",
        "            imagined_rnn_hiddens[0] = flatten_rnn_hiddens\n",
        "\n",
        "            for i in range(1, cfg.imagination_horizon + 1):\n",
        "                i_actions, i_action_log_probs, i_action_entropys = actor(flatten_states, flatten_rnn_hiddens)\n",
        "\n",
        "                flatten_rnn_hiddens = rssm.recurrent(flatten_states, i_actions, flatten_rnn_hiddens)\n",
        "                flatten_states_prior = rssm.get_prior(flatten_rnn_hiddens)\n",
        "                flatten_states = flatten_states_prior.rsample().flatten(1)\n",
        "\n",
        "                imagined_rnn_hiddens[i] = flatten_rnn_hiddens\n",
        "                imagined_states[i] = flatten_states\n",
        "                imagined_action_log_probs[i-1] = i_action_log_probs\n",
        "                imagined_action_entropys[i-1] = i_action_entropys\n",
        "\n",
        "            imagined_states = imagined_states[1:]\n",
        "            imagined_rnn_hiddens = imagined_rnn_hiddens[1:]\n",
        "\n",
        "            flatten_imagined_states = imagined_states.view(-1, cfg.state_dim * cfg.num_classes)\n",
        "            flatten_imagined_rnn_hiddens = imagined_rnn_hiddens.view(-1, cfg.rnn_hidden_dim)\n",
        "\n",
        "            imagined_rewards = reward_model(flatten_imagined_states, flatten_imagined_rnn_hiddens).mean.view(cfg.imagination_horizon, -1)\n",
        "            target_values = target_critic(flatten_imagined_states, flatten_imagined_rnn_hiddens).view(cfg.imagination_horizon, -1).detach()\n",
        "\n",
        "            discount_arr = (cfg.discount * torch.ones_like(imagined_rewards)).to(device)\n",
        "\n",
        "            lambda_target = calculate_lambda_target(imagined_rewards, discount_arr, target_values, cfg.lambda_)\n",
        "\n",
        "            weights = torch.cumprod(torch.cat([torch.ones_like(discount_arr[:1]), discount_arr[:-1]], dim=0), dim=0)\n",
        "            weights[-1] = 0.0\n",
        "\n",
        "            objective = lambda_target + cfg.actor_entropy_scale * imagined_action_entropys\n",
        "            actor_loss = -(weights * objective).mean()\n",
        "\n",
        "            actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            clip_grad_norm_(actor.parameters(), cfg.gradient_clipping)\n",
        "            actor_optimizer.step()\n",
        "\n",
        "            value_mean = critic(flatten_imagined_states.detach(), flatten_imagined_rnn_hiddens.detach()).view(cfg.imagination_horizon, -1)\n",
        "            value_dist = MSE(value_mean)\n",
        "            critic_loss = -(weights.detach() * value_dist.log_prob(lambda_target.detach())).mean()\n",
        "\n",
        "            critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            clip_grad_norm_(critic.parameters(), cfg.gradient_clipping)\n",
        "            critic_optimizer.step()\n",
        "\n",
        "            if args.wandb:\n",
        "                 wandb.log({\n",
        "                     \"Train/Actor_Loss\": actor_loss.item(),\n",
        "                     \"Train/Critic_Loss\": critic_loss.item()\n",
        "                 }, step=iteration)\n",
        "\n",
        "            if (iteration + 1) % cfg.slow_critic_update == 0:\n",
        "                target_critic.load_state_dict(critic.state_dict())\n",
        "\n",
        "    # Save Model\n",
        "    try:\n",
        "        agent.to(\"cpu\")\n",
        "        torch.save(agent, \"agent_lpm.pth\")\n",
        "        print(\"Model saved to agent_lpm.pth\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurring: {e}\")\n",
        "    finally:\n",
        "        print(\"Closing environments...\")\n",
        "        env.close()\n",
        "        eval_env.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dkk99AXJ70od"
      },
      "source": [
        "## 4. Googleドライブのマウント\n",
        "マウントする際は、３でコラボ上にできたファイルを指定したパスに移動してください。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QyNrv-Hc70oe",
        "outputId": "49aca900-1a4c-4077-ac6c-36d5d0130211"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azjNeY0g70oe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "# Google Drive内のプロジェクトフォルダへのパスを指定してください\n",
        "# 例: /content/drive/MyDrive/path/to/your/project\n",
        "# マウントしたら３でコラボ上にできたファイルを移動してください。\n",
        "project_path = '/content/drive/MyDrive/WorldModel_group7'\n",
        "\n",
        "try:\n",
        "    os.chdir(project_path)\n",
        "    print(f\"カレントディレクトリを次に変更しました: {os.getcwd()}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"エラー: パスが見つかりません。プロジェクトフォルダのパスを確認してください: {project_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_hhl0wi70of"
      },
      "source": [
        "## 5. WandB ログイン\n",
        "今回は実行しない。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFNpePYp70of"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-s2Urolc70og"
      },
      "source": [
        "## 6. 学習の実行\n",
        "今回は、wandbは使用しない。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf9zfODu70og",
        "outputId": "1b409666-837f-4a9d-cf89-ed4d9799d977"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "A.L.E: Arcade Learning Environment (version 0.11.2+ecc1138)\n",
            "[Powered by Stella]\n",
            "Pre-filling buffer...\n",
            "Starting Main Loop...\n",
            "Training Steps:   1% 323/50000 [00:07<29:09, 28.39step/s, total_r=0.0, wm_loss=700.33]Episode 1 ExtReward: 0.0031\n",
            "Training Steps:   1% 691/50000 [00:14<12:38, 64.99step/s, total_r=0.0, wm_loss=616.26]Episode 2 ExtReward: 0.0000\n",
            "Training Steps:   3% 1441/50000 [00:31<29:06, 27.80step/s, total_r=0.0, wm_loss=369.26]Episode 3 ExtReward: 0.0013\n",
            "Training Steps:   4% 2182/50000 [00:47<19:51, 40.12step/s, total_r=0.0, wm_loss=218.21]Episode 4 ExtReward: 0.0041\n",
            "Training Steps:   6% 2846/50000 [01:01<15:03, 52.17step/s, total_r=0.0, wm_loss=164.31]Episode 5 ExtReward: 0.0045\n",
            "Training Steps:   7% 3329/50000 [01:11<11:46, 66.08step/s, total_r=0.0, wm_loss=168.37]Episode 6 ExtReward: 0.0041\n",
            "Training Steps:   7% 3676/50000 [01:19<09:03, 85.25step/s, total_r=0.0, wm_loss=138.92]Episode 7 ExtReward: 0.0000\n",
            "Training Steps:   8% 4114/50000 [01:29<15:24, 49.64step/s, total_r=0.0, wm_loss=129.66]Episode 8 ExtReward: 0.0000\n",
            "Training Steps:   9% 4542/50000 [01:37<10:20, 73.28step/s, total_r=0.0, wm_loss=118.29]Episode 9 ExtReward: 0.0023\n",
            "Training Steps:  10% 5069/50000 [01:49<16:53, 44.34step/s, total_r=0.0, wm_loss=111.18]Episode 10 ExtReward: 0.0019\n",
            "Training Steps:  11% 5748/50000 [02:04<10:37, 69.43step/s, total_r=0.0, wm_loss=107.50]Episode 11 ExtReward: 0.0015\n",
            "Training Steps:  13% 6313/50000 [02:16<08:45, 83.18step/s, total_r=0.0, wm_loss=95.13]Episode 12 ExtReward: 0.0018\n",
            "Training Steps:  13% 6725/50000 [02:26<26:38, 27.06step/s, total_r=0.0, wm_loss=93.48]Episode 13 ExtReward: 0.0024\n",
            "Training Steps:  14% 7220/50000 [02:37<20:13, 35.26step/s, total_r=0.0, wm_loss=82.07]Episode 14 ExtReward: 0.0040\n",
            "Training Steps:  16% 7779/50000 [02:50<20:48, 33.80step/s, total_r=0.0, wm_loss=76.55]Episode 15 ExtReward: 0.0018\n",
            "Training Steps:  17% 8322/50000 [03:02<26:10, 26.54step/s, total_r=0.0, wm_loss=84.45]Episode 16 ExtReward: 0.0037\n",
            "Training Steps:  17% 8730/50000 [03:11<23:30, 29.25step/s, total_r=0.0, wm_loss=79.49]Episode 17 ExtReward: 0.0000\n",
            "Training Steps:  18% 9121/50000 [03:20<25:53, 26.32step/s, total_r=0.0, wm_loss=77.74]Episode 18 ExtReward: 0.0052\n",
            "Training Steps:  19% 9647/50000 [03:31<10:39, 63.13step/s, total_r=0.0, wm_loss=86.79]Episode 19 ExtReward: 0.0019\n",
            "Training Steps:  20% 9989/50000 [03:39<08:11, 81.37step/s, total_r=0.0, wm_loss=85.34]Saved combined video to eval_view/video/eval_iter_9999_ep_0.mp4\n",
            "Training Steps:  20% 9989/50000 [03:50<08:11, 81.37step/s, total_r=0.0, wm_loss=85.34]Eval(iter=9999) mean: 0.0000 max: 0.0000\n",
            "Training Steps:  21% 10263/50000 [04:05<15:24, 43.00step/s, total_r=0.0, wm_loss=74.32]Episode 20 ExtReward: 0.0016\n",
            "Training Steps:  21% 10702/50000 [04:14<10:33, 61.99step/s, total_r=0.0, wm_loss=79.79]Episode 21 ExtReward: 0.0022\n",
            "Training Steps:  23% 11273/50000 [04:27<09:11, 70.23step/s, total_r=0.0, wm_loss=83.13]Episode 22 ExtReward: 0.0000\n",
            "Training Steps:  24% 11786/50000 [04:39<16:55, 37.62step/s, total_r=0.0, wm_loss=83.82]Episode 23 ExtReward: 0.0020\n",
            "Training Steps:  25% 12282/50000 [04:50<11:36, 54.17step/s, total_r=0.0, wm_loss=87.00]Episode 24 ExtReward: 0.0020\n",
            "Training Steps:  26% 12796/50000 [05:01<07:39, 80.95step/s, total_r=0.0, wm_loss=56.27]Episode 25 ExtReward: 0.0079\n",
            "Training Steps:  27% 13514/50000 [05:17<07:23, 82.23step/s, total_r=0.0, wm_loss=51.28]Episode 26 ExtReward: 0.0014\n",
            "Training Steps:  28% 14135/50000 [05:31<09:07, 65.54step/s, total_r=0.0, wm_loss=60.33]Episode 27 ExtReward: 0.0016\n",
            "Training Steps:  30% 14820/50000 [05:48<20:06, 29.15step/s, total_r=0.0, wm_loss=53.90]Episode 28 ExtReward: 0.0029\n",
            "Training Steps:  31% 15372/50000 [06:00<20:17, 28.44step/s, total_r=0.0, wm_loss=43.77]Episode 29 ExtReward: 0.0000\n",
            "Training Steps:  32% 15938/50000 [06:13<18:53, 30.06step/s, total_r=0.0, wm_loss=35.40]Episode 30 ExtReward: 0.0035\n",
            "Training Steps:  33% 16396/50000 [06:23<06:29, 86.31step/s, total_r=0.0, wm_loss=46.55]Episode 31 ExtReward: 0.0000\n",
            "Training Steps:  34% 16816/50000 [06:32<15:55, 34.73step/s, total_r=0.0, wm_loss=35.95]Episode 32 ExtReward: 0.0024\n",
            "Training Steps:  35% 17480/50000 [06:47<10:03, 53.85step/s, total_r=0.0, wm_loss=31.59]Episode 33 ExtReward: 0.0000\n",
            "Training Steps:  36% 17886/50000 [06:56<09:49, 54.49step/s, total_r=0.0, wm_loss=40.04]Episode 34 ExtReward: 0.0000\n",
            "Training Steps:  37% 18386/50000 [07:07<07:23, 71.24step/s, total_r=0.0, wm_loss=44.07]Episode 35 ExtReward: 0.0061\n",
            "Training Steps:  38% 18960/50000 [07:21<19:38, 26.35step/s, total_r=0.0, wm_loss=32.10]Episode 36 ExtReward: 0.0070\n",
            "Training Steps:  39% 19331/50000 [07:28<08:55, 57.22step/s, total_r=0.0, wm_loss=33.36]Episode 37 ExtReward: 0.0027\n",
            "Training Steps:  40% 19837/50000 [07:40<06:19, 79.43step/s, total_r=0.0, wm_loss=39.09]Episode 38 ExtReward: 0.0039\n",
            "Training Steps:  40% 19989/50000 [07:43<06:14, 80.21step/s, total_r=0.0, wm_loss=37.14]Saved combined video to eval_view/video/eval_iter_19999_ep_0.mp4\n",
            "Training Steps:  40% 19989/50000 [08:00<06:14, 80.21step/s, total_r=0.0, wm_loss=37.14]Eval(iter=19999) mean: 0.0020 max: 0.0040\n",
            "Training Steps:  41% 20459/50000 [08:50<06:51, 71.85step/s, total_r=0.0, wm_loss=25.22]Episode 39 ExtReward: 0.0048\n",
            "Training Steps:  42% 20937/50000 [09:01<07:50, 61.82step/s, total_r=0.0, wm_loss=32.27]Episode 40 ExtReward: 0.0042\n",
            "Training Steps:  43% 21442/50000 [09:12<22:17, 21.36step/s, total_r=0.0, wm_loss=26.89]Episode 41 ExtReward: 0.0020\n",
            "Training Steps:  44% 21850/50000 [09:21<17:30, 26.79step/s, total_r=0.0, wm_loss=19.44]Episode 42 ExtReward: 0.0024\n",
            "Training Steps:  45% 22391/50000 [09:34<05:40, 80.99step/s, total_r=0.0, wm_loss=20.94]Episode 43 ExtReward: 0.0018\n",
            "Training Steps:  46% 22942/50000 [09:45<06:05, 74.06step/s, total_r=0.0, wm_loss=16.12]Episode 44 ExtReward: 0.0018\n",
            "Training Steps:  47% 23384/50000 [09:56<10:34, 41.92step/s, total_r=0.0, wm_loss=17.09]Episode 45 ExtReward: 0.0044\n",
            "Training Steps:  48% 23911/50000 [10:07<05:20, 81.49step/s, total_r=0.0, wm_loss=14.63]Episode 46 ExtReward: 0.0019\n",
            "Training Steps:  49% 24387/50000 [10:18<06:26, 66.25step/s, total_r=0.0, wm_loss=13.10]Episode 47 ExtReward: 0.0042\n",
            "Training Steps:  50% 24947/50000 [10:31<06:10, 67.56step/s, total_r=0.0, wm_loss=12.15]Episode 48 ExtReward: 0.0018\n",
            "Training Steps:  51% 25498/50000 [10:43<06:16, 65.06step/s, total_r=0.0, wm_loss=11.80]Episode 49 ExtReward: 0.0036\n",
            "Training Steps:  52% 25862/50000 [10:52<09:36, 41.90step/s, total_r=0.0, wm_loss=11.97]Episode 50 ExtReward: 0.0056\n",
            "Training Steps:  53% 26314/50000 [11:01<04:58, 79.38step/s, total_r=0.0, wm_loss=10.51]Episode 51 ExtReward: 0.0022\n",
            "Training Steps:  54% 26981/50000 [11:17<09:10, 41.78step/s, total_r=0.0, wm_loss=9.31]Episode 52 ExtReward: 0.0030\n",
            "Training Steps:  56% 27772/50000 [11:35<14:21, 25.80step/s, total_r=0.0, wm_loss=8.43]Episode 53 ExtReward: 0.0025\n",
            "Training Steps:  57% 28422/50000 [11:50<09:17, 38.68step/s, total_r=0.0, wm_loss=8.18]Episode 54 ExtReward: 0.0031\n",
            "Training Steps:  58% 29028/50000 [12:03<04:27, 78.27step/s, total_r=0.0, wm_loss=7.56]Episode 55 ExtReward: 0.0050\n",
            "Training Steps:  59% 29602/50000 [12:17<13:20, 25.47step/s, total_r=0.0, wm_loss=8.17]Episode 56 ExtReward: 0.0070\n",
            "Training Steps:  60% 29991/50000 [12:25<04:40, 71.22step/s, total_r=0.0, wm_loss=7.04]Saved combined video to eval_view/video/eval_iter_29999_ep_0.mp4\n",
            "Training Steps:  60% 29991/50000 [12:40<04:40, 71.22step/s, total_r=0.0, wm_loss=7.04]Eval(iter=29999) mean: 0.0000 max: 0.0000\n",
            "Training Steps:  60% 30200/50000 [13:01<06:51, 48.08step/s, total_r=0.0, wm_loss=7.64]Episode 57 ExtReward: 0.0000\n",
            "Training Steps:  61% 30733/50000 [13:13<09:44, 32.94step/s, total_r=0.0, wm_loss=7.82]Episode 58 ExtReward: 0.0019\n",
            "Training Steps:  63% 31384/50000 [13:28<07:30, 41.34step/s, total_r=0.0, wm_loss=7.77]Episode 59 ExtReward: 0.0031\n",
            "Training Steps:  64% 31997/50000 [13:42<03:42, 80.92step/s, total_r=0.0, wm_loss=6.44]Episode 60 ExtReward: 0.0033\n",
            "Training Steps:  65% 32380/50000 [13:51<03:59, 73.48step/s, total_r=0.0, wm_loss=6.85]Episode 61 ExtReward: 0.0026\n",
            "Training Steps:  66% 32837/50000 [14:02<05:37, 50.91step/s, total_r=0.0, wm_loss=6.38]Episode 62 ExtReward: 0.0022\n",
            "Training Steps:  66% 33156/50000 [14:09<05:14, 53.64step/s, total_r=0.0, wm_loss=6.56]Episode 63 ExtReward: 0.0032\n",
            "Training Steps:  67% 33649/50000 [14:20<04:21, 62.44step/s, total_r=0.0, wm_loss=6.11]Episode 64 ExtReward: 0.0000\n",
            "Training Steps:  68% 34182/50000 [14:32<06:21, 41.47step/s, total_r=0.0, wm_loss=6.64]Episode 65 ExtReward: 0.0037\n",
            "Training Steps:  69% 34641/50000 [14:43<09:51, 25.95step/s, total_r=0.0, wm_loss=6.93]Episode 66 ExtReward: 0.0066\n",
            "Training Steps:  70% 35050/50000 [14:52<07:57, 31.33step/s, total_r=0.0, wm_loss=6.27]Episode 67 ExtReward: 0.0025\n",
            "Training Steps:  71% 35486/50000 [15:02<04:24, 54.92step/s, total_r=0.0, wm_loss=6.43]Episode 68 ExtReward: 0.0023\n",
            "Training Steps:  72% 36062/50000 [15:15<03:31, 65.79step/s, total_r=0.0, wm_loss=5.40]Episode 69 ExtReward: 0.0017\n",
            "Training Steps:  73% 36454/50000 [15:24<03:45, 60.19step/s, total_r=0.0, wm_loss=6.48]Episode 70 ExtReward: 0.0026\n",
            "Training Steps:  74% 36914/50000 [15:35<04:07, 52.84step/s, total_r=0.0, wm_loss=5.81]Episode 71 ExtReward: 0.0021\n",
            "Training Steps:  75% 37282/50000 [15:44<11:22, 18.64step/s, total_r=0.0, wm_loss=6.44]Episode 72 ExtReward: 0.0027\n",
            "Training Steps:  76% 37771/50000 [15:55<07:37, 26.72step/s, total_r=0.0, wm_loss=6.49]Episode 73 ExtReward: 0.0020\n",
            "Training Steps:  77% 38362/50000 [16:08<03:33, 54.47step/s, total_r=0.0, wm_loss=5.57]Episode 74 ExtReward: 0.0017\n",
            "Training Steps:  78% 38915/50000 [16:20<03:37, 50.99step/s, total_r=0.0, wm_loss=5.70]Episode 75 ExtReward: 0.0036\n",
            "Training Steps:  79% 39413/50000 [16:31<02:46, 63.74step/s, total_r=0.0, wm_loss=5.64]Episode 76 ExtReward: 0.0040\n",
            "Training Steps:  80% 39884/50000 [16:42<03:14, 52.06step/s, total_r=0.0, wm_loss=5.99]Episode 77 ExtReward: 0.0000\n",
            "Training Steps:  80% 39999/50000 [16:44<02:07, 78.40step/s, total_r=0.0, wm_loss=5.80]Saved combined video to eval_view/video/eval_iter_39999_ep_0.mp4\n",
            "Training Steps:  80% 39999/50000 [17:00<02:07, 78.40step/s, total_r=0.0, wm_loss=5.80]Eval(iter=39999) mean: 0.0003 max: 0.0013\n",
            "Training Steps:  81% 40480/50000 [17:21<06:04, 26.10step/s, total_r=0.0, wm_loss=6.08]Episode 78 ExtReward: 0.0051\n",
            "Training Steps:  82% 41165/50000 [17:36<02:21, 62.41step/s, total_r=0.1, wm_loss=6.03]Episode 79 ExtReward: 0.0029\n",
            "Training Steps:  84% 41894/50000 [17:53<02:17, 58.76step/s, total_r=0.0, wm_loss=6.62]Episode 80 ExtReward: 0.0014\n",
            "Training Steps:  85% 42447/50000 [18:05<02:20, 53.74step/s, total_r=0.0, wm_loss=6.46]Episode 81 ExtReward: 0.0036\n",
            "Training Steps:  86% 42998/50000 [18:18<02:48, 41.58step/s, total_r=0.0, wm_loss=5.45]Episode 82 ExtReward: 0.0000\n",
            "Training Steps:  87% 43743/50000 [18:35<01:26, 72.62step/s, total_r=0.1, wm_loss=6.09]Episode 83 ExtReward: 0.0040\n",
            "Training Steps:  89% 44322/50000 [18:48<03:37, 26.11step/s, total_r=0.0, wm_loss=6.00]Episode 84 ExtReward: 0.0017\n",
            "Training Steps:  90% 44864/50000 [19:00<01:13, 69.53step/s, total_r=0.0, wm_loss=5.60]Episode 85 ExtReward: 0.0055\n",
            "Training Steps:  90% 45240/50000 [19:09<01:40, 47.57step/s, total_r=0.0, wm_loss=5.90]Episode 86 ExtReward: 0.0027\n",
            "Training Steps:  91% 45608/50000 [19:18<02:43, 26.86step/s, total_r=0.0, wm_loss=5.38]Episode 87 ExtReward: 0.0027\n",
            "Training Steps:  92% 46197/50000 [19:31<01:12, 52.71step/s, total_r=0.0, wm_loss=5.66]Episode 88 ExtReward: 0.0051\n",
            "Training Steps:  93% 46615/50000 [19:40<00:52, 64.15step/s, total_r=0.0, wm_loss=5.53]Episode 89 ExtReward: 0.0024\n",
            "Training Steps:  94% 47171/50000 [19:52<00:44, 63.57step/s, total_r=0.0, wm_loss=5.46]Episode 90 ExtReward: 0.0036\n",
            "Training Steps:  95% 47527/50000 [20:01<01:38, 25.06step/s, total_r=0.0, wm_loss=5.98]Episode 91 ExtReward: 0.0000\n",
            "Training Steps:  96% 48023/50000 [20:12<00:46, 42.80step/s, total_r=0.0, wm_loss=6.33]Episode 92 ExtReward: 0.0000\n",
            "Training Steps:  97% 48402/50000 [20:21<01:01, 25.93step/s, total_r=0.0, wm_loss=5.63]Episode 93 ExtReward: 0.0026\n",
            "Training Steps:  98% 48846/50000 [20:31<00:19, 59.38step/s, total_r=0.0, wm_loss=5.40]Episode 94 ExtReward: 0.0000\n",
            "Training Steps:  99% 49383/50000 [20:44<00:17, 34.45step/s, total_r=0.0, wm_loss=5.67]Episode 95 ExtReward: 0.0019\n",
            "Training Steps: 100% 49988/50000 [20:57<00:00, 75.01step/s, total_r=0.0, wm_loss=5.65]Saved combined video to eval_view/video/eval_iter_49999_ep_0.mp4\n",
            "Training Steps: 100% 49988/50000 [21:10<00:00, 75.01step/s, total_r=0.0, wm_loss=5.65]Eval(iter=49999) mean: 0.0012 max: 0.0034\n",
            "Training Steps: 100% 50000/50000 [21:23<00:00, 38.97step/s, total_r=0.0, wm_loss=5.69]\n",
            "Model saved to agent_lpm.pth\n",
            "Closing environments...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['MUJOCO_GL'] = 'egl'\n",
        "\n",
        "# 学習を実行\n",
        "!python train_intrinsic.py --env-name \"ALE/BankHeist-v5\" --steps 50000 #--wandb --wandb-project 'Dreamer-Intrinsic-Reward' --wandb-run-name 'breakout-intrinsic-v1'"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}