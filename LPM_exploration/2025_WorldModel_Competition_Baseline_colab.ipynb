{"cells":[{"cell_type":"markdown","metadata":{"id":"HfmtORdYMUTo"},"source":["# 2025年 世界モデル コンペティション1 ベースライン\n","**最低限の要件**  \n","- 行動を出力する agent は引数として obs (numpy.ndarray) を受け取り，action (float) と prior を用いた観測の再構成画像 (numpy.ndarray) を返すように実装する．\n","- 評価時に行動を出力する際には `agent(obs)` と実装している．他の引数を定義しても問題ないが，評価時には他の引数を指定できないため，デフォルトの設定で評価モードにしておく必要がある．\n","\n","**書き換え可能箇所**  \n","- 準備: 必要なライブラリの追加．ただし提出ファイルの作成に google drive を参照するためマウントは削除しないでください．\n","- モデルの実装・学習: 利用したいアルゴリズム及びモデルに自由に変更して構いません．ただしエージェントの入出力の形式は要件を満たしてください．\n","\n","**書き換えてはいけない箇所**  \n","- 環境の設定: omnicampus 上の採点で利用する環境のため，修正しないでください．\n","- 補助機能の実装: エージェントが要件を満たしているか検証する関数になっています．\n","\n","\n","## 目次\n","### モデルの学習と保存\n","1. [準備]()\n","2. [環境の設定]()\n","3. [補助機能の実装]()\n","4. [モデルの実装]()\n","5. [モデルの学習]()\n","6. [エージェントの保存]()\n","7. [student_code.py の作成]()\n","\n","### 提出物の作成\n","8. [submission_tool の準備]()\n","9. [提出物の作成]()\n","10. [提出内容のプレビュー]()"]},{"cell_type":"markdown","metadata":{"id":"yDN8Ohc0Mh8C"},"source":["## 1. 準備"]},{"cell_type":"markdown","metadata":{"id":"LXOA_CGxNmTe"},"source":["必要なライブラリのインストール．各自必要なライブラリがある場合は追加でインストールしてください．  "]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20649,"status":"ok","timestamp":1762149212419,"user":{"displayName":"guch1120","userId":"08277471228501584593"},"user_tz":-540},"id":"FZaErbWsMOOB","outputId":"a674679a-3565-48b6-c29d-2461effe6c92"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting metaworld\n","  Downloading metaworld-3.0.0-py3-none-any.whl.metadata (9.7 kB)\n","Collecting pyvirtualdisplay\n","  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n","Requirement already satisfied: gymnasium>=1.1 in /usr/local/lib/python3.12/dist-packages (from metaworld) (1.2.1)\n","Collecting mujoco>=3.0.0 (from metaworld)\n","  Downloading mujoco-3.3.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.18 in /usr/local/lib/python3.12/dist-packages (from metaworld) (2.0.2)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from metaworld) (1.16.3)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (from metaworld) (2.37.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.1->metaworld) (3.1.1)\n","Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.1->metaworld) (4.15.0)\n","Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium>=1.1->metaworld) (0.0.4)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mujoco>=3.0.0->metaworld) (1.4.0)\n","Requirement already satisfied: etils[epath] in /usr/local/lib/python3.12/dist-packages (from mujoco>=3.0.0->metaworld) (1.13.0)\n","Collecting glfw (from mujoco>=3.0.0->metaworld)\n","  Downloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl.metadata (5.4 kB)\n","Requirement already satisfied: pyopengl in /usr/local/lib/python3.12/dist-packages (from mujoco>=3.0.0->metaworld) (3.1.10)\n","Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.12/dist-packages (from imageio->metaworld) (11.3.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=3.0.0->metaworld) (2025.3.0)\n","Requirement already satisfied: importlib_resources in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=3.0.0->metaworld) (6.5.2)\n","Requirement already satisfied: zipp in /usr/local/lib/python3.12/dist-packages (from etils[epath]->mujoco>=3.0.0->metaworld) (3.23.0)\n","Downloading metaworld-3.0.0-py3-none-any.whl (36.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.7/36.7 MB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n","Downloading mujoco-3.3.7-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading glfw-2.10.0-py2.py27.py3.py30.py31.py32.py33.py34.py35.py36.py37.py38.p39.p310.p311.p312.p313-none-manylinux_2_28_x86_64.whl (243 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.5/243.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pyvirtualdisplay, glfw, mujoco, metaworld\n","Successfully installed glfw-2.10.0 metaworld-3.0.0 mujoco-3.3.7 pyvirtualdisplay-3.0\n"]}],"source":["# ライブラリインストール\n","!pip install metaworld pyvirtualdisplay"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"80DvrvXsPknl","executionInfo":{"status":"ok","timestamp":1762149220432,"user_tz":-540,"elapsed":8003,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["# colab / ubuntu用\n","# 仮想ディスプレイの設定（レンダリング用）\n","!apt install -y xvfb > /dev/null 2>&1"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":636,"status":"ok","timestamp":1762149221070,"user":{"displayName":"guch1120","userId":"08277471228501584593"},"user_tz":-540},"id":"VhXGgnTKPsZK","outputId":"63fe5f78-95c7-465a-ffa6-bf19517ebf6b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyvirtualdisplay.display.Display at 0x7e34f0400f50>"]},"metadata":{},"execution_count":3}],"source":["from pyvirtualdisplay import Display\n","virtual_display = Display(visible=0, size=(400, 300))\n","virtual_display.start()"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":10102,"status":"ok","timestamp":1762149231173,"user":{"displayName":"guch1120","userId":"08277471228501584593"},"user_tz":-540},"id":"72zXgw6dMZ3_","colab":{"base_uri":"https://localhost:8080/"},"outputId":"b20d69c2-2a39-4143-d1a0-69a5faf5c108"},"outputs":[{"output_type":"stream","name":"stderr","text":["Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n","Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n","See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n","/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.utcnow().replace(tzinfo=utc)\n","/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n","  return datetime.utcnow().replace(tzinfo=utc)\n"]}],"source":["import time\n","import os\n","import gc\n","import random\n","from typing import Any, List, Tuple\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","import gym\n","from gym.wrappers import ResizeObservation\n","import metaworld\n","from metaworld.env_dict import (\n","    ALL_V3_ENVIRONMENTS_GOAL_OBSERVABLE,\n","    ALL_V3_ENVIRONMENTS_GOAL_HIDDEN,\n",")\n","import torch\n","import torch.distributions as td\n","from torch.distributions import Normal, OneHotCategoricalStraightThrough\n","from torch.distributions.kl import kl_divergence\n","from torch import nn\n","from torch.nn import functional as F\n","from torch.nn.utils import clip_grad_norm_"]},{"cell_type":"markdown","metadata":{"id":"_eayBK1HPV5H"},"source":["## 2. 環境の設定  \n","**環境の設定については修正しないでください**  \n","- こちらで実装している環境を用いてOmnicampus上では評価を行います．  "]},{"cell_type":"code","execution_count":5,"metadata":{"id":"Pj-DQRVWCJwe","executionInfo":{"status":"ok","timestamp":1762149231179,"user_tz":-540,"elapsed":3,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["class GymWrapperMetaWorld(object):\n","    \"\"\"\n","    MetaWorld環境のためのラッパー\n","    \"\"\"\n","\n","    metadata = {\"render.modes\": [\"human\", \"rgb_array\"]}\n","    reward_range = (-np.inf, np.inf)\n","\n","    # __init__でカメラ位置に関するパラメータ（ cam_dist:カメラ距離，cam_yaw：カメラの水平面での回転，cam_pitch:カメラの縦方向での回転）を受け取り，カメラの位置を調整できるようにします.\n","    # 　同時に画像の大きさも変更できるようにします\n","    def __init__(\n","        self,\n","        name,\n","        seed=None,\n","        size=(64, 64)\n","    ) -> None:\n","        # os.environ[\"MUJOCO_GL\"] = \"egl\"\n","        os.environ[\"MUJOCO_GL\"] = \"glfw\"\n","\n","        task = f\"{name}-v3-goal-observable\"\n","        env_cls = ALL_V3_ENVIRONMENTS_GOAL_OBSERVABLE[task]\n","        self._env = env_cls(seed=seed, render_mode=\"rgb_array\")\n","        self._env.mujoco_renderer.camera_id = 1  # corner に相当\n","        self._env.mujoco_renderer.camera_name = None\n","        self._size = size\n","\n","    def __getattr(self, name: str) -> Any:\n","        return getattr(self._env, name)\n","\n","    @property\n","    def observation_space(self) -> gym.spaces.Box:\n","        width, height = self._size\n","        return gym.spaces.Box(0, 255, (height, width, 3), dtype=np.uint8)\n","\n","    @property\n","    def action_space(self) -> gym.spaces.Box:\n","        return self._env.action_space\n","\n","    # 　元の観測（低次元の状態）は今回は捨てて，env.render()で取得した画像を観測とします.\n","    #  画像，報酬，終了シグナルが得られます.\n","    def step(self, action: np.ndarray) -> (np.ndarray, float, bool, dict):\n","        _, reward, done, truncated, info = self._env.step(action)\n","        obs = self._env.mujoco_renderer.render(render_mode=\"rgb_array\")\n","        obs = np.flipud(obs)\n","        return obs, reward, done, truncated, info\n","\n","    def reset(self) -> np.ndarray:\n","        self._env.reset()\n","        obs = self._env.mujoco_renderer.render(render_mode=\"rgb_array\")\n","        obs = np.flipud(obs)\n","        return obs\n","\n","    def close(self) -> None:\n","        self._env.close()"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"KWcf0WHzPvw-","executionInfo":{"status":"ok","timestamp":1762149231184,"user_tz":-540,"elapsed":2,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["class RepeatAction(gym.Wrapper):\n","    \"\"\"\n","    同じ行動を指定された回数自動的に繰り返すラッパー. 観測は最後の行動に対応するものになる\n","    \"\"\"\n","    def __init__(self, env, skip=4, max_steps=100_000):\n","        gym.Wrapper.__init__(self, env)\n","        self.max_steps = max_steps if max_steps else float(\"inf\")  # イテレーションの制限\n","        self.steps = 0  # イテレーション回数のカウント\n","        self.height = env.observation_space.shape[0]\n","        self.width = env.observation_space.shape[1]\n","        self._skip = skip\n","\n","    def reset(self):\n","        obs = self.env.reset()\n","        return obs\n","\n","    def step(self, action):\n","        if self.steps >= self.max_steps:\n","            print(\"Reached max iterations.\")\n","            return None\n","\n","        total_reward = 0.0\n","        self.steps += 1\n","        for _ in range(self._skip):\n","            obs, reward, done, truncated, info = self.env.step(action)\n","\n","            total_reward += reward\n","            if self.steps >= self.max_steps:\n","                done = True\n","\n","            if done or truncated:\n","                break\n","\n","        return obs, total_reward, done, truncated, info"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"q18Zg5DmSKhe","executionInfo":{"status":"ok","timestamp":1762149231187,"user_tz":-540,"elapsed":2,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["def make_env(env, seed=None, img_size=64, max_steps=None):\n","    # シード固定\n","    env.action_space.seed(seed)\n","    env.observation_space.seed(seed)\n","    env = ResizeObservation(env, (img_size, img_size))\n","    env = RepeatAction(env=env, skip=1, max_steps=max_steps)\n","\n","    return env"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1135,"status":"ok","timestamp":1762149232323,"user":{"displayName":"guch1120","userId":"08277471228501584593"},"user_tz":-540},"id":"vp4v1_DpCMGQ","outputId":"3f48a654-9fd8-49c4-9a55-43b1c3eb5a1c"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]}],"source":["env = GymWrapperMetaWorld(\"hammer\", seed=0, size=(64, 64))\n","env = make_env(env, seed=0)"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"elapsed":5852,"status":"ok","timestamp":1762149238177,"user":{"displayName":"guch1120","userId":"08277471228501584593"},"user_tz":-540},"id":"oc3j4akPChtQ","outputId":"77eb7a00-45ed-4c46-860d-8b5e6cd2b932"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7e33dd92c2c0>"]},"metadata":{},"execution_count":9},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["obs = env.reset()\n","plt.imshow(obs)"]},{"cell_type":"markdown","metadata":{"id":"UzkkdGoQSP6Q"},"source":["## 3. 補助機能の実装  \n","- set_seed: torchのシード値を固定できる関数です．   \n","- check_output_type: エージェントが要件を満たしているか検証する関数です．"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"kwFmpA5qSb8I","executionInfo":{"status":"ok","timestamp":1762149238181,"user_tz":-540,"elapsed":2,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["def set_seed(seed: int) -> None:\n","    \"\"\"\n","    Pytorch, NumPyのシード値を固定します．これによりモデル学習の再現性を担保できます．\n","\n","    Parameters\n","    ----------\n","    seed : int\n","        シード値．\n","    \"\"\"\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"u3SbQNwZMsQf","executionInfo":{"status":"ok","timestamp":1762149238202,"user_tz":-540,"elapsed":9,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["def check_output_type(agent, env):\n","    \"\"\"\n","    agent の入出力が要件を満たしているか検証する関数．\n","    agent は入力として環境からの観測をそのまま受取，環境に直接渡せる形式で出力を作成する必要がある．\n","    （テストケースも兼ねており，この関数で動作すれば採点環境でも動作する想定）\n","    \"\"\"\n","    action_dim = env.action_space.shape[0]\n","    obs = env.reset()\n","    action, pred_obs = agent(obs)\n","\n","    # int 型 + 行動次元に収まっているか確認\n","    assert action.shape[0] == action_dim and (np.abs(action) <= 1).all(), \"行動の出力形式を満たしていません．(shape (4, ), -1 <= action <= 1)\"\n","    assert pred_obs.shape[2] == 3 and (pred_obs >= 0).all() and (pred_obs <= 1).all(), \"観測の出力形式を満たしていません．(shape (64, 64, 3), 0 <= obs <= 1)\"\n","    _, reward, done, _, _ = env.step(action)\n","\n","    print(\"要件を満たしています．\")"]},{"cell_type":"markdown","metadata":{"id":"B3rWEGtkA3e_"},"source":["## 4. モデルの実装\n","**モデルの実装は他のアルゴリズム・モデルに変更していただいて構いません**  \n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","source":["class MSE(td.Normal):\n","    def __init__(self, loc, validate_args=None):\n","        super(MSE, self).__init__(loc, 1.0, validate_args=validate_args)\n","\n","    @property\n","    def mode(self):\n","        return self.mean\n","\n","    def sample(self, sample_shape=torch.Size()):\n","        return self.rsample(sample_shape)\n","\n","    def log_prob(self, value):\n","        if self._validate_args:\n","            self._validate_sample(value)\n","        # NOTE: dropped the constant term\n","        return -((value - self.loc) ** 2) / 2\n","\n","# From https://github.com/toshas/torch_truncnorm/blob/main/TruncatedNormal.py\n","import math\n","from numbers import Number\n","\n","import torch\n","from torch.distributions import Distribution, constraints\n","from torch.distributions.utils import broadcast_all\n","\n","CONST_SQRT_2 = math.sqrt(2)\n","CONST_INV_SQRT_2PI = 1 / math.sqrt(2 * math.pi)\n","CONST_INV_SQRT_2 = 1 / math.sqrt(2)\n","CONST_LOG_INV_SQRT_2PI = math.log(CONST_INV_SQRT_2PI)\n","CONST_LOG_SQRT_2PI_E = 0.5 * math.log(2 * math.pi * math.e)\n","\n","\n","class TruncatedStandardNormal(Distribution):\n","    \"\"\"\n","    Truncated Standard Normal distribution\n","    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n","    \"\"\"\n","\n","    arg_constraints = {\n","        'a': constraints.real,\n","        'b': constraints.real,\n","    }\n","    has_rsample = True\n","\n","    def __init__(self, a, b, validate_args=None):\n","        self.a, self.b = broadcast_all(a, b)\n","        if isinstance(a, Number) and isinstance(b, Number):\n","            batch_shape = torch.Size()\n","        else:\n","            batch_shape = self.a.size()\n","        super(TruncatedStandardNormal, self).__init__(batch_shape, validate_args=validate_args)\n","        if self.a.dtype != self.b.dtype:\n","            raise ValueError('Truncation bounds types are different')\n","        if any((self.a >= self.b).view(-1, ).tolist()):\n","            raise ValueError('Incorrect truncation range')\n","        eps = torch.finfo(self.a.dtype).eps\n","        self._dtype_min_gt_0 = eps\n","        self._dtype_max_lt_1 = 1 - eps\n","        self._little_phi_a = self._little_phi(self.a)\n","        self._little_phi_b = self._little_phi(self.b)\n","        self._big_phi_a = self._big_phi(self.a)\n","        self._big_phi_b = self._big_phi(self.b)\n","        self._Z = (self._big_phi_b - self._big_phi_a).clamp_min(eps)\n","        self._log_Z = self._Z.log()\n","        little_phi_coeff_a = torch.nan_to_num(self.a, nan=math.nan)\n","        little_phi_coeff_b = torch.nan_to_num(self.b, nan=math.nan)\n","        self._lpbb_m_lpaa_d_Z = (self._little_phi_b * little_phi_coeff_b -\n","                                 self._little_phi_a * little_phi_coeff_a) / self._Z\n","        self._mean = -(self._little_phi_b - self._little_phi_a) / self._Z\n","        # NOTE: additional to github.com/toshas/torch_truncnorm\n","        self._mode = torch.clamp(torch.zeros_like(self.a), self.a, self.b)\n","        self._variance = 1 - self._lpbb_m_lpaa_d_Z - ((self._little_phi_b - self._little_phi_a) / self._Z) ** 2\n","        self._entropy = CONST_LOG_SQRT_2PI_E + self._log_Z - 0.5 * self._lpbb_m_lpaa_d_Z\n","\n","    @constraints.dependent_property\n","    def support(self):\n","        return constraints.interval(self.a, self.b)\n","\n","    @property\n","    def mean(self):\n","        return self._mean\n","\n","    @property\n","    def mode(self):\n","        return self._mode\n","\n","    @property\n","    def variance(self):\n","        return self._variance\n","\n","    def entropy(self):\n","        return self._entropy\n","\n","    @property\n","    def auc(self):\n","        return self._Z\n","\n","    @staticmethod\n","    def _little_phi(x):\n","        return (-(x ** 2) * 0.5).exp() * CONST_INV_SQRT_2PI\n","\n","    @staticmethod\n","    def _big_phi(x):\n","        return 0.5 * (1 + (x * CONST_INV_SQRT_2).erf())\n","\n","    @staticmethod\n","    def _inv_big_phi(x):\n","        return CONST_SQRT_2 * (2 * x - 1).erfinv()\n","\n","    def cdf(self, value):\n","        if self._validate_args:\n","            self._validate_sample(value)\n","        return ((self._big_phi(value) - self._big_phi_a) / self._Z).clamp(0, 1)\n","\n","    def icdf(self, value):\n","        return self._inv_big_phi(self._big_phi_a + value * self._Z)\n","\n","    def log_prob(self, value):\n","        if self._validate_args:\n","            self._validate_sample(value)\n","        return CONST_LOG_INV_SQRT_2PI - self._log_Z - (value ** 2) * 0.5\n","\n","    def rsample(self, sample_shape=torch.Size()):\n","        # icdf is numerically unstable; as a consequence, so is rsample.\n","        shape = self._extended_shape(sample_shape)\n","        p = torch.empty(shape, device=self.a.device).uniform_(self._dtype_min_gt_0, self._dtype_max_lt_1)\n","        return self.icdf(p)\n","\n","\n","class TruncatedNormal(TruncatedStandardNormal):\n","    \"\"\"\n","    Truncated Normal distribution\n","    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n","    \"\"\"\n","\n","    has_rsample = True\n","\n","    def __init__(self, loc, scale, scalar_a, scalar_b, validate_args=None):\n","        self.loc, self.scale, a, b = broadcast_all(loc, scale, scalar_a, scalar_b)\n","        a = (a - self.loc) / self.scale\n","        b = (b - self.loc) / self.scale\n","        super(TruncatedNormal, self).__init__(a, b, validate_args=validate_args)\n","        self._log_scale = self.scale.log()\n","        self._mean = self._mean * self.scale + self.loc\n","        self._mode = torch.clamp(self.loc, scalar_a, scalar_b)  # NOTE: additional to github.com/toshas/torch_truncnorm\n","        self._variance = self._variance * self.scale ** 2\n","        self._entropy += self._log_scale\n","\n","    def _to_std_rv(self, value):\n","        return (value - self.loc) / self.scale\n","\n","    def _from_std_rv(self, value):\n","        return value * self.scale + self.loc\n","\n","    def cdf(self, value):\n","        return super(TruncatedNormal, self).cdf(self._to_std_rv(value))\n","\n","    def icdf(self, value):\n","        return self._from_std_rv(super(TruncatedNormal, self).icdf(value))\n","\n","    def log_prob(self, value):\n","        return super(TruncatedNormal, self).log_prob(self._to_std_rv(value)) - self._log_scale\n","\n","\n","class TruncNormalDist(TruncatedNormal):\n","\n","    def __init__(self, loc, scale, low, high, clip=1e-6, mult=1):\n","        super().__init__(loc, scale, low, high)\n","        self._clip = clip\n","        self._mult = mult\n","\n","        self.low = low\n","        self.high = high\n","\n","    def sample(self, *args, **kwargs):\n","        event = super().rsample(*args, **kwargs)\n","        if self._clip:\n","            clipped = torch.clamp(\n","                event, self.low + self._clip, self.high - self._clip\n","            )\n","            event = event - event.detach() + clipped.detach()\n","        if self._mult:\n","            event *= self._mult\n","        return event"],"metadata":{"id":"KudCq6cLeRzp","executionInfo":{"status":"ok","timestamp":1762151735944,"user_tz":-540,"elapsed":11,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"execution_count":45,"outputs":[]},{"cell_type":"code","execution_count":46,"metadata":{"id":"bUt9Lp3HMeJq","executionInfo":{"status":"ok","timestamp":1762151738877,"user_tz":-540,"elapsed":9,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["class RSSM(nn.Module):\n","    def __init__(self, mlp_hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int, actino_dim: int):\n","        super().__init__()\n","\n","        self.rnn_hidden_dim = rnn_hidden_dim\n","        self.state_dim = state_dim\n","        self.num_classes = num_classes\n","\n","        # Recurrent model\n","        # h_t = f(h_t-1, z_t-1, a_t-1)\n","        self.transition_hidden = nn.Linear(state_dim * num_classes + action_dim, mlp_hidden_dim)\n","        self.transition = nn.GRUCell(mlp_hidden_dim, rnn_hidden_dim)\n","\n","        # transition predictor\n","        self.prior_hidden = nn.Linear(rnn_hidden_dim, mlp_hidden_dim)\n","        self.prior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n","\n","        # representation model\n","        self.posterior_hidden = nn.Linear(rnn_hidden_dim + 1536, mlp_hidden_dim)\n","        self.posterior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n","\n","    def recurrent(self, state: torch.Tensor, action: torch.Tensor, rnn_hidden: torch.Tensor):\n","        # recullent model: h_t = f(h_t-1, z_t-1, a_t-1)を計算する\n","        hidden = F.elu(self.transition_hidden(torch.cat([state, action], dim=1)))\n","        rnn_hidden = self.transition(hidden, rnn_hidden)\n","\n","        return rnn_hidden  # h_t\n","\n","    def get_prior(self, rnn_hidden: torch.Tensor, detach=False):\n","        # transition predictor: \\hat{z}_t ~ p(z\\hat{z}_t | h_t)\n","        hidden = F.elu(self.prior_hidden(rnn_hidden))\n","        logits = self.prior_logits(hidden)\n","        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n","\n","        prior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n","        if detach:\n","            detach_prior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n","            return prior_dist, detach_prior  # p(z\\hat{z}_t | h_t)\n","        return prior_dist\n","\n","    def get_posterior(self, rnn_hidden: torch.Tensor, embedded_obs: torch.Tensor, detach=False):\n","        # representation predictor: z_t ~ q(z_t | h_t, o_t)\n","        hidden = F.elu(self.posterior_hidden(torch.cat([rnn_hidden, embedded_obs], dim=1)))\n","        logits = self.posterior_logits(hidden)\n","        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n","\n","        posterior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n","        if detach:\n","            detach_posterior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n","            return posterior_dist, detach_posterior  # q(z_t | h_t, o_t)\n","        return posterior_dist"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"VmsUzBFYMeJq","executionInfo":{"status":"ok","timestamp":1762149238304,"user_tz":-540,"elapsed":11,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["class Encoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(3, 48, kernel_size=4, stride=2)\n","        self.conv2 = nn.Conv2d(48, 96, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(96, 192, kernel_size=4, stride=2)\n","        self.conv4 = nn.Conv2d(192, 384, kernel_size=4, stride=2)\n","\n","    def forward(self, obs: torch.Tensor):\n","        \"\"\"\n","        観測画像をベクトルに埋め込むためのEncoder．\n","\n","        Parameters\n","        ----------\n","        obs : torch.Tensor (B, C, H, W)\n","            入力となる観測画像．\n","\n","        Returns\n","        -------\n","        embedded_obs : torch.Tensor (B, D)\n","            観測画像をベクトルに変換したもの．Dは入力画像の幅と高さに依存して変わる．\n","            入力が(B, 3, 64, 64)の場合，出力は(B, 1536)になる．\n","        \"\"\"\n","        hidden = F.elu(self.conv1(obs))\n","        hidden = F.elu(self.conv2(hidden))\n","        hidden = F.elu(self.conv3(hidden))\n","        embedded_obs = self.conv4(hidden).reshape(hidden.size(0), -1)\n","\n","        return embedded_obs  # x_t"]},{"cell_type":"code","execution_count":47,"metadata":{"id":"4c0l4yGSo9Fk","executionInfo":{"status":"ok","timestamp":1762151741370,"user_tz":-540,"elapsed":9,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["class Decoder(nn.Module):\n","    def __init__(self, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","        self.fc = nn.Linear(state_dim*num_classes + rnn_hidden_dim, 1536)\n","        self.dc1 = nn.ConvTranspose2d(1536, 192, kernel_size=5, stride=2)\n","        self.dc2 = nn.ConvTranspose2d(192, 96, kernel_size=5, stride=2)\n","        self.dc3 = nn.ConvTranspose2d(96, 48, kernel_size=6, stride=2)\n","        self.dc4 = nn.ConvTranspose2d(48, 3, kernel_size=6, stride=2)\n","\n","\n","    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        決定論的状態と，確率的状態を入力として，観測画像を復元するDecoder．\n","        出力は多次元正規分布の平均値をとる．\n","\n","        Paremters\n","        ---------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        obs_dist : torch.distribution.Independent\n","            観測画像を再構成するための多次元正規分布．\n","        \"\"\"\n","        hidden = self.fc(torch.cat([state, rnn_hidden], dim=1))\n","        hidden = hidden.view(hidden.size(0), 1536, 1, 1)\n","        hidden = F.elu(self.dc1(hidden))\n","        hidden = F.elu(self.dc2(hidden))\n","        hidden = F.elu(self.dc3(hidden))\n","        mean = self.dc4(hidden)\n","\n","        obs_dist = td.Independent(MSE(mean), 3)\n","        return obs_dist  # p(\\hat{x}_t | h_t, z_t)"]},{"cell_type":"code","execution_count":48,"metadata":{"id":"uJ5OpjS4deuO","executionInfo":{"status":"ok","timestamp":1762151743018,"user_tz":-540,"elapsed":19,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["class RewardModel(nn.Module):\n","    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        決定論的状態と，確率的状態を入力として，報酬を予測するモデル．\n","        出力は正規分布の平均値をとる．\n","\n","        Paremters\n","        ---------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        reward_dist : torch.distribution.Independent\n","            報酬を予測するための正規分布．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        mean = self.fc4(hidden)\n","\n","        reward_dist = td.Independent(MSE(mean),  1)\n","        return reward_dist  # p(\\hat{r}_t | h_t, z_t)"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"QzJB9ZiOxMnn","executionInfo":{"status":"ok","timestamp":1762151744340,"user_tz":-540,"elapsed":10,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["class DiscountModel(nn.Module):\n","    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        決定論的状態と，確率的状態を入力として，現在の状態がエピソード終端かどうか判別するモデル．\n","        出力はベルヌーイ分布の平均値をとる．\n","\n","        Paremters\n","        ---------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        discount_dist : torch.distribution.Independent\n","            状態が終端かどうかを予測するためのベルヌーイ分布．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        mean= self.fc4(hidden)\n","\n","        discount_dist = td.Independent(td.Bernoulli(logits=mean),  1)\n","        return discount_dist  # p(\\hat{\\gamma}_t | h_t, z_t)"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"EyvUF6YHdk4v","executionInfo":{"status":"ok","timestamp":1762151745773,"user_tz":-540,"elapsed":9,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["class Actor(nn.Module):\n","    def __init__(self, action_dim: int, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","\n","        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n","        self.mean = nn.Linear(hidden_dim, action_dim)\n","        self.std = nn.Linear(hidden_dim, action_dim)\n","        self.min_stddev = 0.1\n","        self.init_stddev = np.log(np.exp(5.0) - 1)\n","\n","    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor, eval: bool = False):\n","        \"\"\"\n","        確率的状態を入力として，criticで推定される価値が最大となる行動を出力する．\n","\n","        Parameters\n","        ----------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        action : torch.Tensor (B, 1)\n","            行動．\n","        action_log_prob : torch.Tensor(B, 1)\n","            予測した行動をとる確率の対数．\n","        action_entropy : torch.Tensor(B, 1)\n","            予測した確率分布のエントロピー．エントロピー正則化に使用．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        hidden = F.elu(self.fc4(hidden))\n","        mean = self.mean(hidden)\n","        stddev = self.std(hidden)\n","\n","        mean = torch.tanh(mean)\n","        stddev = 2 * torch.sigmoid((stddev + self.init_stddev) / 2) + self.min_stddev\n","        if eval:\n","            action = mean\n","            return action, None, None\n","\n","        action_dist = td.Independent(TruncNormalDist(mean, stddev, -1, 1), 1)  # 行動をサンプリングする分布: p_{\\psi} (\\hat{a}_t | \\hat{z}_t)\n","        action = action_dist.sample()  # 行動: \\hat{a}_t\n","\n","        action_log_prob = action_dist.log_prob(action)\n","        action_entropy = action_dist.entropy()\n","\n","        return action, action_log_prob, action_entropy"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"JRdJzb-YdsW6","executionInfo":{"status":"ok","timestamp":1762151747305,"user_tz":-540,"elapsed":4,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["class Critic(nn.Module):\n","    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","\n","        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n","        self.out = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        確率的状態を入力として，価値関数(lambda target)の値を予測する．．\n","\n","        Parameters\n","        ----------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        value : torch.Tensor (B, 1)\n","            入力された状態に対する状態価値関数の予測値．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        hidden = F.elu(self.fc4(hidden))\n","        mean = self.out(hidden)\n","\n","        return mean"]},{"cell_type":"code","execution_count":52,"metadata":{"id":"tmptCR67aYZc","executionInfo":{"status":"ok","timestamp":1762151748539,"user_tz":-540,"elapsed":10,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["class ReplayBuffer(object):\n","    \"\"\"\n","    RNNを用いて訓練するのに適したリプレイバッファ\n","    \"\"\"\n","    def __init__(self, capacity, observation_shape, action_dim):\n","        self.capacity = capacity\n","\n","        self.observations = np.zeros((capacity, *observation_shape), dtype=np.float32)\n","        self.actions = np.zeros((capacity, action_dim), dtype=np.float32)\n","        self.rewards = np.zeros((capacity, 1), dtype=np.float32)\n","        self.done = np.zeros((capacity, 1), dtype=bool)\n","\n","        self.index = 0\n","        self.is_filled = False\n","\n","    def push(self, observation, action, reward, done):\n","        \"\"\"\n","        リプレイバッファに経験を追加する\n","        \"\"\"\n","        self.observations[self.index] = observation\n","        self.actions[self.index] = action\n","        self.rewards[self.index] = reward\n","        self.done[self.index] = done\n","\n","        # indexは巡回し, 最も古い経験を上書きする\n","        if self.index == self.capacity - 1:\n","            self.is_filled = True\n","        self.index = (self.index + 1) % self.capacity\n","\n","    def sample(self, batch_size, chunk_length):\n","        \"\"\"\n","        経験をリプレイバッファからサンプルします. （ほぼ）一様なサンプルです\n","        結果として返ってくるのは観測(画像), 行動, 報酬, 終了シグナルについての(batch_size, chunk_length, 各要素の次元)の配列です\n","        各バッチは連続した経験になっています\n","        注意: chunk_lengthをあまり大きな値にすると問題が発生する場合があります\n","        \"\"\"\n","        episode_borders = np.where(self.done)[0]\n","        sampled_indexes = []\n","        for _ in range(batch_size):\n","            cross_border = True\n","            while cross_border:\n","                initial_index = np.random.randint(len(self) - chunk_length + 1)\n","                final_index = initial_index + chunk_length - 1\n","                cross_border = np.logical_and(initial_index <= episode_borders,\n","                                              episode_borders < final_index).any()#論理積\n","            sampled_indexes += list(range(initial_index, final_index + 1))\n","\n","        sampled_observations = self.observations[sampled_indexes].reshape(\n","            batch_size, chunk_length, *self.observations.shape[1:])\n","        sampled_actions = self.actions[sampled_indexes].reshape(\n","            batch_size, chunk_length, self.actions.shape[1])\n","        sampled_rewards = self.rewards[sampled_indexes].reshape(\n","            batch_size, chunk_length, 1)\n","        sampled_done = self.done[sampled_indexes].reshape(\n","            batch_size, chunk_length, 1)\n","        return sampled_observations, sampled_actions, sampled_rewards, sampled_done\n","\n","    def __len__(self):\n","        return self.capacity if self.is_filled else self.index\n","\n","    def save(self, dir: str):\n","        np.save(f\"{dir}/observations\", self.observations)\n","        np.save(f\"{dir}/actions\", self.actions)\n","        np.save(f\"{dir}/rewards\", self.rewards)\n","        np.save(f\"{dir}/done\", self.done)\n","\n","    def load(self, dir: str):\n","        self.observations = np.load(f\"{dir}/observations.npy\")\n","        self.actions = np.load(f\"{dir}/actions.npy\")\n","        self.rewards = np.load(f\"{dir}/rewards.npy\")\n","        self.done = np.load(f\"{dir}/done.npy\")"]},{"cell_type":"code","execution_count":53,"metadata":{"id":"MLIt0qiya1_2","executionInfo":{"status":"ok","timestamp":1762151750176,"user_tz":-540,"elapsed":9,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["def preprocess_obs(obs):\n","    \"\"\"\n","    画像の変換. [0, 255] -> [-0.5, 0.5]\n","    \"\"\"\n","    obs = obs.astype(np.float32)\n","    normalized_obs = obs / 255.0 - 0.5\n","    return normalized_obs"]},{"cell_type":"code","execution_count":54,"metadata":{"id":"fLZMKJj_bIHr","executionInfo":{"status":"ok","timestamp":1762151751713,"user_tz":-540,"elapsed":10,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["def calculate_lambda_target(rewards: torch.Tensor, discounts: torch.Tensor, values: torch.Tensor, lambda_: float):\n","    \"\"\"\n","    lambda targetを計算する関数．\n","\n","    Parameters\n","    ---------\n","    rewards : torch.Tensor (imagination_horizon, D)\n","        報酬．1次元目が時刻tを表しており，2次元目は自由な次元数にでき，想像の軌道を作成するときに入力されるサンプルindexと考える．\n","    discounts : torch.Tensor (imagination_horizon, D)\n","        割引率．gammaそのままを利用するのではなく，DiscountModelの出力をかけて利用する．\n","    values : torch.Tensor (imagination_horizon, D)\n","        状態価値関数．criticで予測された値を利用するが，Dreamer v2ではtarget networkで計算する．\n","    lambda_ : float\n","        lambda targetのハイパラ．\n","\n","    Returns\n","    -------\n","    V_lambda : torch.Tensor (imagination_horizon, D)\n","        lambda targetの値．\n","    \"\"\"\n","    V_lambda = torch.zeros_like(rewards)\n","\n","    for t in reversed(range(rewards.shape[0])):\n","        if t == rewards.shape[0] - 1:\n","            V_lambda[t] = rewards[t] + discounts[t] * values[t]  # t=Hの場合（式4の下の条件）\n","        else:\n","            V_lambda[t] = rewards[t] + discounts[t] * ((1-lambda_) * values[t+1] + lambda_ * V_lambda[t+1])\n","\n","    return V_lambda"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"kGY-YZzmSh-3","executionInfo":{"status":"ok","timestamp":1762151753156,"user_tz":-540,"elapsed":15,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["class Agent(nn.Module):\n","    \"\"\"\n","    ActionModelに基づき行動を決定する. そのためにRSSMを用いて状態表現をリアルタイムで推論して維持するクラス\n","    \"\"\"\n","    def __init__(self, encoder, decoder, rssm, action_model):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.rssm = rssm\n","        self.action_model = action_model\n","\n","        self.device = next(self.action_model.parameters()).device\n","        self.rnn_hidden = torch.zeros(1, rssm.rnn_hidden_dim, device=self.device)\n","\n","    def __call__(self, obs, eval=True):\n","        # preprocessを適用, PyTorchのためにChannel-Firstに変換\n","        obs = preprocess_obs(obs)\n","        obs = torch.as_tensor(obs, device=self.device)\n","        obs = obs.transpose(1, 2).transpose(0, 1).unsqueeze(0)\n","\n","        with torch.no_grad():\n","            # 現在の状態から次に得られる観測画像を予測する\n","            state_prior = self.rssm.get_prior(self.rnn_hidden)\n","            state = state_prior.sample().flatten(1)\n","            obs_dist = self.decoder(state, self.rnn_hidden)\n","            obs_pred = obs_dist.mean\n","\n","            # 観測を低次元の表現に変換し, posteriorからのサンプルをActionModelに入力して行動を決定する\n","            embedded_obs = self.encoder(obs)\n","            state_posterior = self.rssm.get_posterior(self.rnn_hidden, embedded_obs)\n","            state = state_posterior.sample().flatten(1)\n","            action, _, _  = self.action_model(state, self.rnn_hidden, eval=eval)\n","\n","            # 次のステップのためにRNNの隠れ状態を更新しておく\n","            self.rnn_hidden = self.rssm.recurrent(state, action, self.rnn_hidden)\n","\n","        return action.squeeze().cpu().numpy(), (obs_pred.squeeze().cpu().numpy().transpose(1, 2, 0) + 0.5).clip(0.0, 1.0)\n","\n","    #RNNの隠れ状態をリセット\n","    def reset(self):\n","        self.rnn_hidden = torch.zeros(1, self.rssm.rnn_hidden_dim, device=self.device)\n","\n","    def to(self, device):\n","        self.device = device\n","        self.encoder.to(device)\n","        self.decoder.to(device)\n","        self.rssm.to(device)\n","        self.action_model.to(device)\n","        self.rnn_hidden = self.rnn_hidden.to(device)"]},{"cell_type":"markdown","metadata":{"id":"QUPWAkpyc9Z-"},"source":["## 5. モデルの学習  \n","**アルゴリズム・モデルに合わせて修正いただいて構いません．**"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"TMKhjMfcfvwi","executionInfo":{"status":"ok","timestamp":1762151755295,"user_tz":-540,"elapsed":8,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["class Config:\n","    def __init__(self, **kwargs):\n","        # コメントアウトされている値は，元実装のハイパーパラメータの値\n","        # data settings\n","        self.buffer_size = 100_000  # バッファにためるデータの上限\n","        self.batch_size = 16  # 50  # 学習時のバッチサイズ\n","        self.seq_length = 50  # 各バッチの系列長\n","        self.imagination_horizon = 10  # 15  # 想像上の軌道の系列長\n","\n","        # model dimensions\n","        self.state_dim = 32  # 32  # 確率的な状態の次元数\n","        self.num_classes = 32  # 32  # 確率的な状態のクラス数（離散表現のため）\n","        self.rnn_hidden_dim = 400  # 600  # 決定論的な状態の次元数\n","        self.mlp_hidden_dim = 300  # 400  # MLPの隠れ層の次元数\n","\n","        # learning params\n","        self.model_lr = 2e-4  # world model(transition / prior / posterior / discount / image predictor)の学習率\n","        self.actor_lr = 4e-5  # actorの学習率\n","        self.critic_lr = 1e-4  # criticの学習率\n","        self.epsilon = 1e-5  # optimizerのepsilonの値\n","        self.weight_decay = 1e-6  # weight decayの係数\n","        self.gradient_clipping = 100  # 勾配クリッピング\n","        self.kl_scale = 0.1  # kl lossのスケーリング係数\n","        self.kl_balance = 0.8  # kl balancingの係数(fix posterior)\n","        self.actor_entropy_scale = 1e-3  # entropy正則化のスケーリング係数\n","        self.slow_critic_update = 100  # target critic networkの更新頻度\n","        self.reward_loss_scale = 1.0  # reward lossのスケーリング係数\n","        self.discount_loss_scale = 1.0  # discount lossのスケーリング係数\n","        self.update_freq = 80  # 4\n","\n","        # lambda return params\n","        self.discount = 0.995  # 割引率\n","        self.lambda_ = 0.95  # lambda returnのパラメータ\n","\n","        # learning period settings\n","        self.iter = 6000  # 総ステップ数（最初のランダム方策含む）\n","        self.seed_iter = 3000  # 事前にランダム行動で探索する回数\n","        self.eval_freq = 5  # 評価頻度（エピソード）\n","        self.eval_episodes = 5  # 評価に用いるエピソード数\n","\n","cfg = Config()"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zzgkGsnaR8X3","executionInfo":{"status":"ok","timestamp":1762151759492,"user_tz":-540,"elapsed":2356,"user":{"displayName":"guch1120","userId":"08277471228501584593"}},"outputId":"bb0e65b8-66b1-40c6-8155-8636009d4695"},"execution_count":57,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":58,"metadata":{"executionInfo":{"elapsed":196,"status":"ok","timestamp":1762151759691,"user":{"displayName":"guch1120","userId":"08277471228501584593"},"user_tz":-540},"id":"Z8sW8K5Kdjrg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"da54505f-4fe2-4d68-cbc7-b4274e206736"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}],"source":["set_seed(1234)  # PyTorchのシード固定\n","env = GymWrapperMetaWorld(\"hammer\", seed=0, size=(64, 64))\n","env = make_env(env, seed=0)\n","\n","eval_env = GymWrapperMetaWorld(\"hammer\", seed=0, size=(64, 64))\n","eval_env = make_env(eval_env, seed=1234)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(device)\n","\n","action_dim = env.action_space.shape[0]\n","# リプレイバッファ\n","replay_buffer = ReplayBuffer(\n","    capacity=cfg.buffer_size,\n","    observation_shape=(64, 64, 3),\n","    action_dim=env.action_space.shape[0],\n",")\n","\n","# モデル\n","rssm = RSSM(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes, action_dim).to(device)\n","encoder = Encoder().to(device)\n","decoder = Decoder(cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n","reward_model =  RewardModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n","# discount_model = DiscountModel(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n","actor = Actor(action_dim, cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n","critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n","target_critic = Critic(cfg.mlp_hidden_dim, cfg.rnn_hidden_dim, cfg.state_dim, cfg.num_classes).to(device)\n","target_critic.load_state_dict(critic.state_dict())\n","\n","# optimizer\n","wm_params = list(rssm.parameters())         + \\\n","            list(encoder.parameters())      + \\\n","            list(decoder.parameters())      + \\\n","            list(reward_model.parameters()) # + \\\n","            # list(discount_model.parameters())\n","\n","wm_optimizer = torch.optim.Adam(wm_params, lr=cfg.model_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n","actor_optimizer = torch.optim.Adam(actor.parameters(), lr=cfg.actor_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)\n","critic_optimizer = torch.optim.Adam(critic.parameters(), lr=cfg.critic_lr, eps=cfg.epsilon, weight_decay=cfg.weight_decay)"]},{"cell_type":"code","execution_count":59,"metadata":{"id":"1IzprukvoiOD","executionInfo":{"status":"ok","timestamp":1762151762749,"user_tz":-540,"elapsed":4,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["def evaluation(eval_env: RepeatAction, policy: Agent, step: int, cfg: Config):\n","    \"\"\"\n","    評価用の関数．\n","\n","    Parameters\n","    ----------\n","    policy : Agent\n","        エージェントのインスタンス．\n","    step : int\n","        現状の訓練のステップ数．\n","    cfg : Config\n","        コンフィグ．\n","\n","    Returns\n","    -------\n","    max_ep_rewards : float\n","        評価中に1エピソードで得た最大の報酬和．\n","    \"\"\"\n","    env = eval_env\n","    all_ep_rewards = []\n","\n","    with torch.no_grad():\n","        for i in range(cfg.eval_episodes):\n","            obs = env.reset()  # 環境をリセット\n","            policy.reset()  # RNNの隠れ状態をリセット\n","            done = False  # 終端条件\n","            truncated = False\n","            episode_reward = []  # エピソードでの報酬和\n","            while not done and not truncated:\n","                action, _ = policy(obs)\n","\n","                obs, reward, done, truncated, info = env.step(action)\n","                episode_reward.append(reward)\n","\n","            if len(episode_reward) < 500:\n","                if info[\"success\"]:\n","                    episode_reward = np.pad(episode_reward, (0, 500 - len(episode_reward)), \"constant\", constant_values=10)\n","                else:\n","                    episode_reward = np.pad(episode_reward, (0, 500 - len(episode_reward)), \"constant\", constant_values=0)\n","\n","            mean_episode_reward = np.mean(episode_reward)\n","            all_ep_rewards.append(mean_episode_reward)\n","\n","        mean_ep_rewards = np.mean(all_ep_rewards)\n","        max_ep_rewards = np.max(all_ep_rewards)\n","        print(f\"Eval(iter={step}) mean: {mean_ep_rewards:.4f} max: {max_ep_rewards:.4f}\")\n","\n","    return max_ep_rewards"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"f8U7uuJgNgKz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762151765846,"user_tz":-540,"elapsed":1240,"user":{"displayName":"guch1120","userId":"08277471228501584593"}},"outputId":"5d32d51a-2870-435b-c638-2ea1b7841cb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["要件を満たしています．\n"]}],"source":["# モデルの要件チェック\n","check_output_type(Agent(encoder, decoder, rssm, actor), env)"]},{"cell_type":"code","execution_count":61,"metadata":{"id":"txKI9CCne5e3","executionInfo":{"status":"ok","timestamp":1762152565069,"user_tz":-540,"elapsed":797799,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["# ランダム行動でバッファを埋める\n","obs = env.reset()\n","done = False\n","for _ in range(cfg.seed_iter):\n","    action = env.action_space.sample()\n","    next_obs, reward, done, truncated, _ = env.step(action)\n","    done = done or truncated\n","\n","    if done or truncated:\n","        replay_buffer.push(preprocess_obs(obs), action, reward, done)\n","        obs = env.reset()\n","        done = False\n","        truncated = False\n","\n","    else:\n","        replay_buffer.push(preprocess_obs(obs), action, reward, done)\n","        obs = next_obs"]},{"cell_type":"code","source":["# pretrain\n","for iteration in range(100):\n","    # モデルの学習\n","    # リプレイバッファからデータをサンプリングする\n","    # (batch size, seq_lenght, *data shape)\n","    observations, actions, rewards, done_flags =\\\n","        replay_buffer.sample(cfg.batch_size, cfg.seq_length)\n","    done_flags = 1 - done_flags  # 終端でない場合に1をとる\n","\n","    # torchで扱える形（seq lengthを最初の次元に，画像はchnnelを最初の次元にする）に変形，observationの前処理\n","    observations = torch.permute(torch.as_tensor(observations, device=device), (1, 0, 4, 2, 3))  # (T, B, C, H, W)\n","    actions = torch.as_tensor(actions, device=device).transpose(0, 1)  # (T, B, action dim)\n","    rewards = torch.as_tensor(rewards, device=device).transpose(0, 1)  # (T, B, 1)\n","    done_flags = torch.as_tensor(done_flags, device=device).transpose(0, 1).float()  # (T, B, 1)\n","\n","    # =================\n","    # world modelの学習\n","    # =================\n","    # 観測をベクトルに埋めこみ\n","    emb_observations = encoder(observations.reshape(-1, 3, 64, 64)).view(cfg.seq_length, cfg.batch_size, -1)  # (T, B, 1536)\n","\n","    # 状態表現z，行動aはゼロで初期化\n","    # バッファから取り出したデータをt={1, ..., seq length}とするなら，以下はz_1とみなせる\n","    state = torch.zeros(cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n","    rnn_hidden = torch.zeros(cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n","\n","    # 各観測に対して状態表現を計算\n","    # タイムステップごとに計算するため，先に格納するTensorを定義する(t={1, ..., seq length})\n","    states = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n","    rnn_hiddens = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n","\n","    # prior, posteriorを計算してKL lossを計算する\n","    kl_loss = 0\n","    for i in range(cfg.seq_length-1):\n","        # rnn hiddenを更新\n","        rnn_hidden = rssm.recurrent(state, actions[i], rnn_hidden)  # h_t+1\n","\n","        # prior, posteriorを計算\n","        next_state_prior, next_detach_prior = rssm.get_prior(rnn_hidden, detach=True) # \\hat{z}_t+1\n","        next_state_posterior, next_detach_posterior = rssm.get_posterior(rnn_hidden, emb_observations[i+1], detach=True)  # z_t+1\n","\n","        # posteriorからzをサンプリング\n","        state = next_state_posterior.rsample().flatten(1)\n","        rnn_hiddens[i+1] = rnn_hidden  # h_t+1\n","        states[i+1] = state  # z_t+1\n","\n","        # KL lossを計算\n","        kl_loss +=  cfg.kl_balance * torch.mean(kl_divergence(next_detach_posterior, next_state_prior)) + \\\n","                    (1 - cfg.kl_balance) * torch.mean(kl_divergence(next_state_posterior, next_detach_prior))\n","    kl_loss /= (cfg.seq_length - 1)\n","\n","    # 初期状態は使わない\n","    rnn_hiddens = rnn_hiddens[1:]  # (seq lenghth - 1, batch size rnn hidden)\n","    states = states[1:]  # (seq length - 1, batch size, state dim * num_classes)\n","\n","    # 得られた状態を利用して再構成，報酬，終端フラグを予測\n","    # そのままでは時間方向，バッチ方向で次元が多いため平坦化\n","    flatten_rnn_hiddens = rnn_hiddens.view(-1, cfg.rnn_hidden_dim)  # ((T-1) * B, rnn hidden)\n","    flatten_states = states.view(-1, cfg.state_dim * cfg.num_classes)  # ((T-1) * B, state_dim * num_classes)\n","\n","    # 上から再構成，報酬，終端フラグ予測\n","    obs_dist = decoder(flatten_states, flatten_rnn_hiddens)  # (T * B, 3, 64, 64)\n","    reward_dist = reward_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n","    # discount_dist = discount_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n","\n","    # 各予測に対する損失の計算（対数尤度）\n","    C, H, W = observations.shape[2:]\n","    obs_loss = -torch.mean(obs_dist.log_prob(observations[1:].reshape(-1, C, H, W)))\n","    reward_loss = -torch.mean(reward_dist.log_prob(rewards[:-1].reshape(-1, 1)))\n","    # discount_loss = -torch.mean(discount_dist.log_prob(done_flags[:-1].float().reshape(-1, 1)))\n","\n","    # 総和をとってモデルを更新\n","    # wm_loss = obs_loss + cfg.reward_loss_scale * reward_loss + cfg.discount_loss_scale * discount_loss + cfg.kl_scale * kl_loss\n","    wm_loss = obs_loss + cfg.reward_loss_scale * reward_loss + cfg.kl_scale * kl_loss\n","\n","    wm_optimizer.zero_grad()\n","    wm_loss.backward()\n","    clip_grad_norm_(wm_params, cfg.gradient_clipping)\n","    wm_optimizer.step()\n","\n","    #====================\n","    # Actor, Criticの更新\n","    #===================\n","    # wmから得た状態の勾配を切っておく\n","    flatten_rnn_hiddens = flatten_rnn_hiddens.detach()\n","    flatten_states = flatten_states.detach()\n","\n","    # priorを用いた状態予測\n","    # 格納する空のTensorを用意\n","    imagined_states = torch.zeros(cfg.imagination_horizon + 1,\n","                                  *flatten_states.shape,\n","                                  device=flatten_states.device)\n","    imagined_rnn_hiddens = torch.zeros(cfg.imagination_horizon + 1,\n","                                       *flatten_rnn_hiddens.shape,\n","                                       device=flatten_rnn_hiddens.device)\n","    imagined_action_log_probs = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)),\n","                                            device=flatten_rnn_hiddens.device)\n","    imagined_action_entropys = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)),\n","                                            device=flatten_rnn_hiddens.device)\n","\n","    # 未来予測をして想像上の軌道を作る前に, 最初の状態としては先ほどモデルの更新で使っていた\n","    # リプレイバッファからサンプルされた観測データを取り込んだ上で推論した状態表現を使う\n","    imagined_states[0] = flatten_states\n","    imagined_rnn_hiddens[0] = flatten_rnn_hiddens\n","\n","    for i in range(1, cfg.imagination_horizon + 1):\n","        actions, action_log_probs, action_entropys = actor(flatten_states, flatten_rnn_hiddens)  # ((T-1) * B, action dim)\n","\n","        # rnn hiddenを更新, priorで次の状態を予測\n","        flatten_rnn_hiddens = rssm.recurrent(flatten_states, actions, flatten_rnn_hiddens)  # h_t+1\n","        flatten_states_prior = rssm.get_prior(flatten_rnn_hiddens)\n","        flatten_states = flatten_states_prior.rsample().flatten(1)\n","\n","        imagined_rnn_hiddens[i] = flatten_rnn_hiddens\n","        imagined_states[i] = flatten_states\n","        imagined_action_log_probs[i-1] = action_log_probs\n","        imagined_action_entropys[i-1] = action_entropys\n","\n","    imagined_states = imagined_states[1:]\n","    imagined_rnn_hiddens = imagined_rnn_hiddens[1:]\n","\n","    # 得られた状態から報酬を予測\n","    flatten_imagined_states = imagined_states.view(-1, cfg.state_dim * cfg.num_classes)  # ((imagination horizon) * (T-1) * B, state dim * num classes)\n","    flatten_imagined_rnn_hiddens = imagined_rnn_hiddens.view(-1, cfg.rnn_hidden_dim)  # ((imagination horizon) * (T-1) * B, rnn hidden)\n","\n","    # reward, done_flagsは分布なので平均値をとる\n","    # ((imagination horizon + 1), (T-1) * B)\n","    imagined_rewards = reward_model(flatten_imagined_states, flatten_imagined_rnn_hiddens).mean.view(cfg.imagination_horizon, -1)\n","    target_values = target_critic(flatten_imagined_states, flatten_imagined_rnn_hiddens).view(cfg.imagination_horizon, -1).detach()\n","    discount_arr = (cfg.discount * torch.ones_like(imagined_rewards)).to(device)\n","    initial_done = done_flags[1:].reshape(1, -1)\n","    discount_arr[0] = cfg.discount * initial_done\n","\n","    # lambda targetの計算\n","    lambda_target = calculate_lambda_target(imagined_rewards, discount_arr, target_values, cfg.lambda_)\n","\n","    # actorの損失を計算\n","    weights = torch.cumprod(\n","        torch.cat([torch.ones_like(discount_arr[:1]), discount_arr[:-1]], dim=0), dim=0\n","    )\n","    weights[-1] = 0.0\n","    objective = lambda_target + cfg.actor_entropy_scale * imagined_action_entropys\n","    actor_loss = -(weights * objective).mean()\n","\n","    actor_optimizer.zero_grad()\n","    actor_loss.backward()\n","    clip_grad_norm_(actor.parameters(), cfg.gradient_clipping)\n","    actor_optimizer.step()\n","\n","    # criticの損失を計算\n","    value_mean = critic(flatten_imagined_states.detach(), flatten_imagined_rnn_hiddens.detach()).view(cfg.imagination_horizon, -1)\n","    value_dist = MSE(value_mean)\n","    critic_loss = -(weights.detach() * value_dist.log_prob(lambda_target.detach())).mean()\n","\n","    critic_optimizer.zero_grad()\n","    critic_loss.backward()\n","    clip_grad_norm_(critic.parameters(), cfg.gradient_clipping)\n","    critic_optimizer.step()\n","\n","    if (iteration + 1) % cfg.slow_critic_update == 0:\n","        target_critic.load_state_dict(critic.state_dict())"],"metadata":{"id":"KPZGSHbkf1yo","executionInfo":{"status":"ok","timestamp":1762152656996,"user_tz":-540,"elapsed":91925,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","execution_count":63,"metadata":{"id":"6_LNryZQfnKT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762155731617,"user_tz":-540,"elapsed":3074619,"user":{"displayName":"guch1120","userId":"08277471228501584593"}},"outputId":"3b0bc7f0-ec94-4c6e-fb34-f8d7f37ce6a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["episode: 1 mean_episode_reward: 0.94117164\n","num iter: 499 kl loss: 1.35760760 obs loss: 14.22967148 rewrd loss: 0.00789903 critic loss: 10.66788101 actor loss: -9.85811138\n","episode: 2 mean_episode_reward: 0.93558256\n","num iter: 999 kl loss: 6.72083521 obs loss: 15.82107830 rewrd loss: 0.01445081 critic loss: 7.30231571 actor loss: -12.89925194\n","episode: 3 mean_episode_reward: 0.94715854\n","num iter: 1499 kl loss: 9.26336861 obs loss: 16.65489197 rewrd loss: 0.01854482 critic loss: 2.86013794 actor loss: -16.80589867\n","episode: 4 mean_episode_reward: 0.93642700\n","num iter: 1999 kl loss: 14.56439686 obs loss: 13.00034332 rewrd loss: 0.00540692 critic loss: 1.66592419 actor loss: -21.94284821\n","Eval(iter=1999) mean: 0.9239 max: 0.9240\n","episode: 5 mean_episode_reward: 0.94485500\n","num iter: 2499 kl loss: 9.02650928 obs loss: 15.55572891 rewrd loss: 0.00266111 critic loss: 7.60815716 actor loss: -33.13814163\n","episode: 6 mean_episode_reward: 0.96093641\n","num iter: 2999 kl loss: 3.93787766 obs loss: 14.35531616 rewrd loss: 0.00134117 critic loss: 3.90427470 actor loss: -38.64193726\n","episode: 7 mean_episode_reward: 0.94105141\n","num iter: 3499 kl loss: 2.70111084 obs loss: 11.23107052 rewrd loss: 0.00149808 critic loss: 1.43209934 actor loss: -44.04606247\n","episode: 8 mean_episode_reward: 0.94656147\n","num iter: 3999 kl loss: 1.81040764 obs loss: 13.31708622 rewrd loss: 0.00054690 critic loss: 1.93752611 actor loss: -49.52500534\n","episode: 9 mean_episode_reward: 0.94425962\n","num iter: 4499 kl loss: 1.95341384 obs loss: 14.05756474 rewrd loss: 0.00368414 critic loss: 5.44835472 actor loss: -59.74660492\n","Eval(iter=4499) mean: 0.9006 max: 0.9176\n","episode: 10 mean_episode_reward: 0.90849639\n","num iter: 4999 kl loss: 2.33664227 obs loss: 14.01893044 rewrd loss: 0.00029751 critic loss: 3.48274922 actor loss: -63.80015564\n","episode: 11 mean_episode_reward: 1.07366858\n","num iter: 5499 kl loss: 4.13231611 obs loss: 11.46779346 rewrd loss: 0.00373429 critic loss: 0.98220509 actor loss: -67.36442566\n","episode: 12 mean_episode_reward: 0.83860515\n","num iter: 5999 kl loss: 7.91301918 obs loss: 13.20736027 rewrd loss: 0.00664558 critic loss: 1.46837950 actor loss: -72.44850159\n"]}],"source":["# 学習を行う\n","# 環境と相互作用 → 一定イテレーションでモデル更新を繰り返す\n","policy = Agent(encoder, decoder, rssm, actor)\n","\n","# 環境，収益等の初期化\n","obs = env.reset()\n","done = False\n","truncated = False\n","total_reward = []\n","total_episode = 1\n","best_reward = -1\n","\n","for iteration in range(cfg.iter):\n","    with torch.no_grad():\n","        # 環境と相互作用\n","        action, _ = policy(obs, eval=False)  # モデルで行動をサンプリング(one-hot)\n","        next_obs, reward, done,truncated, info = env.step(action)  # 環境を進める\n","        done = done or truncated\n","\n","        # 得たデータをリプレイバッファに追加して更新\n","        replay_buffer.push(preprocess_obs(obs), action, reward, done)  # x_t, a_t, r_t, gamma_t\n","        obs = next_obs\n","        total_reward.append(reward)\n","\n","    if (iteration + 1) % cfg.update_freq == 0:\n","        # モデルの学習\n","        # リプレイバッファからデータをサンプリングする\n","        # (batch size, seq_lenght, *data shape)\n","        observations, actions, rewards, done_flags =\\\n","            replay_buffer.sample(cfg.batch_size, cfg.seq_length)\n","        done_flags = 1 - done_flags  # 終端でない場合に1をとる\n","\n","        # torchで扱える形（seq lengthを最初の次元に，画像はchnnelを最初の次元にする）に変形，observationの前処理\n","        observations = torch.permute(torch.as_tensor(observations, device=device), (1, 0, 4, 2, 3))  # (T, B, C, H, W)\n","        actions = torch.as_tensor(actions, device=device).transpose(0, 1)  # (T, B, action dim)\n","        rewards = torch.as_tensor(rewards, device=device).transpose(0, 1)  # (T, B, 1)\n","        done_flags = torch.as_tensor(done_flags, device=device).transpose(0, 1).float()  # (T, B, 1)\n","\n","        # =================\n","        # world modelの学習\n","        # =================\n","        # 観測をベクトルに埋めこみ\n","        emb_observations = encoder(observations.reshape(-1, 3, 64, 64)).view(cfg.seq_length, cfg.batch_size, -1)  # (T, B, 1536)\n","\n","        # 状態表現z，行動aはゼロで初期化\n","        # バッファから取り出したデータをt={1, ..., seq length}とするなら，以下はz_1とみなせる\n","        state = torch.zeros(cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n","        rnn_hidden = torch.zeros(cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n","\n","        # 各観測に対して状態表現を計算\n","        # タイムステップごとに計算するため，先に格納するTensorを定義する(t={1, ..., seq length})\n","        states = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.state_dim*cfg.num_classes, device=device)\n","        rnn_hiddens = torch.zeros(cfg.seq_length, cfg.batch_size, cfg.rnn_hidden_dim, device=device)\n","\n","        # prior, posteriorを計算してKL lossを計算する\n","        kl_loss = 0\n","        for i in range(cfg.seq_length-1):\n","            # rnn hiddenを更新\n","            rnn_hidden = rssm.recurrent(state, actions[i], rnn_hidden)  # h_t+1\n","\n","            # prior, posteriorを計算\n","            next_state_prior, next_detach_prior = rssm.get_prior(rnn_hidden, detach=True) # \\hat{z}_t+1\n","            next_state_posterior, next_detach_posterior = rssm.get_posterior(rnn_hidden, emb_observations[i+1], detach=True)  # z_t+1\n","\n","            # posteriorからzをサンプリング\n","            state = next_state_posterior.rsample().flatten(1)\n","            rnn_hiddens[i+1] = rnn_hidden  # h_t+1\n","            states[i+1] = state  # z_t+1\n","\n","            # KL lossを計算\n","            kl_loss +=  cfg.kl_balance * torch.mean(kl_divergence(next_detach_posterior, next_state_prior)) + \\\n","                        (1 - cfg.kl_balance) * torch.mean(kl_divergence(next_state_posterior, next_detach_prior))\n","        kl_loss /= (cfg.seq_length - 1)\n","\n","        # 初期状態は使わない\n","        rnn_hiddens = rnn_hiddens[1:]  # (seq lenghth - 1, batch size rnn hidden)\n","        states = states[1:]  # (seq length - 1, batch size, state dim * num_classes)\n","\n","        # 得られた状態を利用して再構成，報酬，終端フラグを予測\n","        # そのままでは時間方向，バッチ方向で次元が多いため平坦化\n","        flatten_rnn_hiddens = rnn_hiddens.view(-1, cfg.rnn_hidden_dim)  # ((T-1) * B, rnn hidden)\n","        flatten_states = states.view(-1, cfg.state_dim * cfg.num_classes)  # ((T-1) * B, state_dim * num_classes)\n","\n","        # 上から再構成，報酬，終端フラグ予測\n","        obs_dist = decoder(flatten_states, flatten_rnn_hiddens)  # (T * B, 3, 64, 64)\n","        reward_dist = reward_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n","        # discount_dist = discount_model(flatten_states, flatten_rnn_hiddens)  # (T * B, 1)\n","\n","        # 各予測に対する損失の計算（対数尤度）\n","        C, H, W = observations.shape[2:]\n","        obs_loss = -torch.mean(obs_dist.log_prob(observations[1:].reshape(-1, C, H, W)))\n","        reward_loss = -torch.mean(reward_dist.log_prob(rewards[:-1].reshape(-1, 1)))\n","        # discount_loss = -torch.mean(discount_dist.log_prob(done_flags[:-1].float().reshape(-1, 1)))\n","\n","        # 総和をとってモデルを更新\n","        # wm_loss = obs_loss + cfg.reward_loss_scale * reward_loss + cfg.discount_loss_scale * discount_loss + cfg.kl_scale * kl_loss\n","        wm_loss = obs_loss + cfg.reward_loss_scale * reward_loss + cfg.kl_scale * kl_loss\n","\n","        wm_optimizer.zero_grad()\n","        wm_loss.backward()\n","        clip_grad_norm_(wm_params, cfg.gradient_clipping)\n","        wm_optimizer.step()\n","\n","        #====================\n","        # Actor, Criticの更新\n","        #===================\n","        # wmから得た状態の勾配を切っておく\n","        flatten_rnn_hiddens = flatten_rnn_hiddens.detach()\n","        flatten_states = flatten_states.detach()\n","\n","        # priorを用いた状態予測\n","        # 格納する空のTensorを用意\n","        imagined_states = torch.zeros(cfg.imagination_horizon + 1,\n","                                      *flatten_states.shape,\n","                                      device=flatten_states.device)\n","        imagined_rnn_hiddens = torch.zeros(cfg.imagination_horizon + 1,\n","                                           *flatten_rnn_hiddens.shape,\n","                                           device=flatten_rnn_hiddens.device)\n","        imagined_action_log_probs = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)),\n","                                                device=flatten_rnn_hiddens.device)\n","        imagined_action_entropys = torch.zeros((cfg.imagination_horizon, cfg.batch_size * (cfg.seq_length-1)),\n","                                                device=flatten_rnn_hiddens.device)\n","\n","        # 未来予測をして想像上の軌道を作る前に, 最初の状態としては先ほどモデルの更新で使っていた\n","        # リプレイバッファからサンプルされた観測データを取り込んだ上で推論した状態表現を使う\n","        imagined_states[0] = flatten_states\n","        imagined_rnn_hiddens[0] = flatten_rnn_hiddens\n","\n","        for i in range(1, cfg.imagination_horizon + 1):\n","            actions, action_log_probs, action_entropys = actor(flatten_states, flatten_rnn_hiddens)  # ((T-1) * B, action dim)\n","\n","            # rnn hiddenを更新, priorで次の状態を予測\n","            flatten_rnn_hiddens = rssm.recurrent(flatten_states, actions, flatten_rnn_hiddens)  # h_t+1\n","            flatten_states_prior = rssm.get_prior(flatten_rnn_hiddens)\n","            flatten_states = flatten_states_prior.rsample().flatten(1)\n","\n","            imagined_rnn_hiddens[i] = flatten_rnn_hiddens\n","            imagined_states[i] = flatten_states\n","            imagined_action_log_probs[i-1] = action_log_probs\n","            imagined_action_entropys[i-1] = action_entropys\n","\n","        imagined_states = imagined_states[1:]\n","        imagined_rnn_hiddens = imagined_rnn_hiddens[1:]\n","\n","        # 得られた状態から報酬を予測\n","        flatten_imagined_states = imagined_states.view(-1, cfg.state_dim * cfg.num_classes)  # ((imagination horizon) * (T-1) * B, state dim * num classes)\n","        flatten_imagined_rnn_hiddens = imagined_rnn_hiddens.view(-1, cfg.rnn_hidden_dim)  # ((imagination horizon) * (T-1) * B, rnn hidden)\n","\n","        # reward, done_flagsは分布なので平均値をとる\n","        # ((imagination horizon + 1), (T-1) * B)\n","        imagined_rewards = reward_model(flatten_imagined_states, flatten_imagined_rnn_hiddens).mean.view(cfg.imagination_horizon, -1)\n","        target_values = target_critic(flatten_imagined_states, flatten_imagined_rnn_hiddens).view(cfg.imagination_horizon, -1).detach()\n","        discount_arr = (cfg.discount * torch.ones_like(imagined_rewards)).to(device)\n","        initial_done = done_flags[1:].reshape(1, -1)\n","        discount_arr[0] = cfg.discount * initial_done\n","\n","        # lambda targetの計算\n","        lambda_target = calculate_lambda_target(imagined_rewards, discount_arr, target_values, cfg.lambda_)\n","\n","        # actorの損失を計算\n","        weights = torch.cumprod(\n","            torch.cat([torch.ones_like(discount_arr[:1]), discount_arr[:-1]], dim=0), dim=0\n","        )\n","        weights[-1] = 0.0\n","        objective = lambda_target + cfg.actor_entropy_scale * imagined_action_entropys\n","        actor_loss = -(weights * objective).mean()\n","\n","        actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        clip_grad_norm_(actor.parameters(), cfg.gradient_clipping)\n","        actor_optimizer.step()\n","\n","        # criticの損失を計算\n","        value_mean = critic(flatten_imagined_states.detach(), flatten_imagined_rnn_hiddens.detach()).view(cfg.imagination_horizon, -1)\n","        value_dist = MSE(value_mean)\n","        critic_loss = -(weights.detach() * value_dist.log_prob(lambda_target.detach())).mean()\n","\n","        critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        clip_grad_norm_(critic.parameters(), cfg.gradient_clipping)\n","        critic_optimizer.step()\n","\n","        if (iteration + 1) % cfg.slow_critic_update == 0:\n","            target_critic.load_state_dict(critic.state_dict())\n","\n","    # エピソードが終了した時に再初期化\n","    if done or truncated:\n","        if len(total_reward) < 500:\n","            if info[\"success\"]:\n","                total_reward = np.pad(total_reward, (0, 500 - len(total_reward)), \"constant\", constant_values=10)\n","            else:\n","                total_reward = np.pad(total_reward, (0, 500 - len(total_reward)), \"constant\", constant_values=0)\n","\n","        mean_episode_reward = np.mean(total_reward)\n","        print(f\"episode: {total_episode} mean_episode_reward: {mean_episode_reward:.8f}\")\n","        print(f\"num iter: {iteration} kl loss: {kl_loss.item():.8f} obs loss: {obs_loss.item():.8f} \"\n","              f\"rewrd loss: {reward_loss.item():.8f} \" # discount loss: {discount_loss.item():.8f} \"\n","              f\"critic loss: {critic_loss.item():.8f} actor loss: {actor_loss.item():.8f}\"\n","        )\n","        obs = env.reset()\n","        done = False\n","        truncated = False\n","        total_reward = []\n","        total_episode += 1\n","        policy.reset()\n","\n","        # 一定エピソードごとに評価\n","        if total_episode % cfg.eval_freq == 0:\n","            eval_reward = evaluation(eval_env, policy, iteration, cfg)\n","            eval_env.reset()\n","            policy.reset()"]},{"cell_type":"markdown","metadata":{"id":"QY1cyyotT0K2"},"source":["## 6. エージェントの保存\n","- 保存する際には，CPU に移してからモデル全体を保存してください．\n","  - state_dict のみの保存に変更しないでください．"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"UWjmC-iMAMOg","executionInfo":{"status":"ok","timestamp":1762155743191,"user_tz":-540,"elapsed":110,"user":{"displayName":"guch1120","userId":"08277471228501584593"}}},"outputs":[],"source":["agent = Agent(encoder, decoder, rssm, actor)  # この行は書き換え可\n","agent.to(\"cpu\")\n","torch.save(agent, \"agent.pth\")"]},{"cell_type":"markdown","source":[],"metadata":{"id":"vKGRxd19mN-7"}},{"cell_type":"markdown","metadata":{"id":"5qaekGawqhzL"},"source":["## 7. sutudent_code.py の作成\n","- `%%writefile student_code.py` はセルの内容を `student_code.py` としてファイル作成するマジックコマンドです．この部分は削除・変更等しないでください．\n","- 学習した Agent のクラスを定義したスクリプトを `student_code.py` として作成する必要があります．\n","- Agent クラス自体と，その内部で動作するクラス・関数をすべて含めてください．\n","- 各クラス・関数に利用されているライブラリもスクリプト内で import してください．\n","    - 不足しているクラス・関数がある場合，secure_submit.py を用いて提出物を作成する際にエラーが発生します．"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"qHl8zUIaV89U","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762149322140,"user_tz":-540,"elapsed":27,"user":{"displayName":"guch1120","userId":"08277471228501584593"}},"outputId":"49d2c992-997a-4b3e-c757-a9536a14b2f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Writing student_code.py\n"]}],"source":["%%writefile student_code.py\n","import numpy as np\n","import torch\n","import torch.distributions as td\n","from torch.distributions import Normal, OneHotCategoricalStraightThrough\n","from torch import nn\n","from torch.nn import functional as F\n","\n","\n","class MSE(td.Normal):\n","    def __init__(self, loc, validate_args=None):\n","        super(MSE, self).__init__(loc, 1.0, validate_args=validate_args)\n","\n","    @property\n","    def mode(self):\n","        return self.mean\n","\n","    def sample(self, sample_shape=torch.Size()):\n","        return self.rsample(sample_shape)\n","\n","    def log_prob(self, value):\n","        if self._validate_args:\n","            self._validate_sample(value)\n","        # NOTE: dropped the constant term\n","        return -((value - self.loc) ** 2) / 2\n","\n","# From https://github.com/toshas/torch_truncnorm/blob/main/TruncatedNormal.py\n","import math\n","from numbers import Number\n","\n","import torch\n","from torch.distributions import Distribution, constraints\n","from torch.distributions.utils import broadcast_all\n","\n","CONST_SQRT_2 = math.sqrt(2)\n","CONST_INV_SQRT_2PI = 1 / math.sqrt(2 * math.pi)\n","CONST_INV_SQRT_2 = 1 / math.sqrt(2)\n","CONST_LOG_INV_SQRT_2PI = math.log(CONST_INV_SQRT_2PI)\n","CONST_LOG_SQRT_2PI_E = 0.5 * math.log(2 * math.pi * math.e)\n","\n","\n","class TruncatedStandardNormal(Distribution):\n","    \"\"\"\n","    Truncated Standard Normal distribution\n","    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n","    \"\"\"\n","\n","    arg_constraints = {\n","        'a': constraints.real,\n","        'b': constraints.real,\n","    }\n","    has_rsample = True\n","\n","    def __init__(self, a, b, validate_args=None):\n","        self.a, self.b = broadcast_all(a, b)\n","        if isinstance(a, Number) and isinstance(b, Number):\n","            batch_shape = torch.Size()\n","        else:\n","            batch_shape = self.a.size()\n","        super(TruncatedStandardNormal, self).__init__(batch_shape, validate_args=validate_args)\n","        if self.a.dtype != self.b.dtype:\n","            raise ValueError('Truncation bounds types are different')\n","        if any((self.a >= self.b).view(-1, ).tolist()):\n","            raise ValueError('Incorrect truncation range')\n","        eps = torch.finfo(self.a.dtype).eps\n","        self._dtype_min_gt_0 = eps\n","        self._dtype_max_lt_1 = 1 - eps\n","        self._little_phi_a = self._little_phi(self.a)\n","        self._little_phi_b = self._little_phi(self.b)\n","        self._big_phi_a = self._big_phi(self.a)\n","        self._big_phi_b = self._big_phi(self.b)\n","        self._Z = (self._big_phi_b - self._big_phi_a).clamp_min(eps)\n","        self._log_Z = self._Z.log()\n","        little_phi_coeff_a = torch.nan_to_num(self.a, nan=math.nan)\n","        little_phi_coeff_b = torch.nan_to_num(self.b, nan=math.nan)\n","        self._lpbb_m_lpaa_d_Z = (self._little_phi_b * little_phi_coeff_b -\n","                                 self._little_phi_a * little_phi_coeff_a) / self._Z\n","        self._mean = -(self._little_phi_b - self._little_phi_a) / self._Z\n","        # NOTE: additional to github.com/toshas/torch_truncnorm\n","        self._mode = torch.clamp(torch.zeros_like(self.a), self.a, self.b)\n","        self._variance = 1 - self._lpbb_m_lpaa_d_Z - ((self._little_phi_b - self._little_phi_a) / self._Z) ** 2\n","        self._entropy = CONST_LOG_SQRT_2PI_E + self._log_Z - 0.5 * self._lpbb_m_lpaa_d_Z\n","\n","    @constraints.dependent_property\n","    def support(self):\n","        return constraints.interval(self.a, self.b)\n","\n","    @property\n","    def mean(self):\n","        return self._mean\n","\n","    @property\n","    def mode(self):\n","        return self._mode\n","\n","    @property\n","    def variance(self):\n","        return self._variance\n","\n","    def entropy(self):\n","        return self._entropy\n","\n","    @property\n","    def auc(self):\n","        return self._Z\n","\n","    @staticmethod\n","    def _little_phi(x):\n","        return (-(x ** 2) * 0.5).exp() * CONST_INV_SQRT_2PI\n","\n","    @staticmethod\n","    def _big_phi(x):\n","        return 0.5 * (1 + (x * CONST_INV_SQRT_2).erf())\n","\n","    @staticmethod\n","    def _inv_big_phi(x):\n","        return CONST_SQRT_2 * (2 * x - 1).erfinv()\n","\n","    def cdf(self, value):\n","        if self._validate_args:\n","            self._validate_sample(value)\n","        return ((self._big_phi(value) - self._big_phi_a) / self._Z).clamp(0, 1)\n","\n","    def icdf(self, value):\n","        return self._inv_big_phi(self._big_phi_a + value * self._Z)\n","\n","    def log_prob(self, value):\n","        if self._validate_args:\n","            self._validate_sample(value)\n","        return CONST_LOG_INV_SQRT_2PI - self._log_Z - (value ** 2) * 0.5\n","\n","    def rsample(self, sample_shape=torch.Size()):\n","        # icdf is numerically unstable; as a consequence, so is rsample.\n","        shape = self._extended_shape(sample_shape)\n","        p = torch.empty(shape, device=self.a.device).uniform_(self._dtype_min_gt_0, self._dtype_max_lt_1)\n","        return self.icdf(p)\n","\n","\n","class TruncatedNormal(TruncatedStandardNormal):\n","    \"\"\"\n","    Truncated Normal distribution\n","    https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n","    \"\"\"\n","\n","    has_rsample = True\n","\n","    def __init__(self, loc, scale, scalar_a, scalar_b, validate_args=None):\n","        self.loc, self.scale, a, b = broadcast_all(loc, scale, scalar_a, scalar_b)\n","        a = (a - self.loc) / self.scale\n","        b = (b - self.loc) / self.scale\n","        super(TruncatedNormal, self).__init__(a, b, validate_args=validate_args)\n","        self._log_scale = self.scale.log()\n","        self._mean = self._mean * self.scale + self.loc\n","        self._mode = torch.clamp(self.loc, scalar_a, scalar_b)  # NOTE: additional to github.com/toshas/torch_truncnorm\n","        self._variance = self._variance * self.scale ** 2\n","        self._entropy += self._log_scale\n","\n","    def _to_std_rv(self, value):\n","        return (value - self.loc) / self.scale\n","\n","    def _from_std_rv(self, value):\n","        return value * self.scale + self.loc\n","\n","    def cdf(self, value):\n","        return super(TruncatedNormal, self).cdf(self._to_std_rv(value))\n","\n","    def icdf(self, value):\n","        return self._from_std_rv(super(TruncatedNormal, self).icdf(value))\n","\n","    def log_prob(self, value):\n","        return super(TruncatedNormal, self).log_prob(self._to_std_rv(value)) - self._log_scale\n","\n","\n","class TruncNormalDist(TruncatedNormal):\n","\n","    def __init__(self, loc, scale, low, high, clip=1e-6, mult=1):\n","        super().__init__(loc, scale, low, high)\n","        self._clip = clip\n","        self._mult = mult\n","\n","        self.low = low\n","        self.high = high\n","\n","    def sample(self, *args, **kwargs):\n","        event = super().rsample(*args, **kwargs)\n","        if self._clip:\n","            clipped = torch.clamp(\n","                event, self.low + self._clip, self.high - self._clip\n","            )\n","            event = event - event.detach() + clipped.detach()\n","        if self._mult:\n","            event *= self._mult\n","        return event\n","\n","\n","class RSSM(nn.Module):\n","    def __init__(self, mlp_hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int, actino_dim: int):\n","        super().__init__()\n","\n","        self.rnn_hidden_dim = rnn_hidden_dim\n","        self.state_dim = state_dim\n","        self.num_classes = num_classes\n","\n","        # Recurrent model\n","        # h_t = f(h_t-1, z_t-1, a_t-1)\n","        self.transition_hidden = nn.Linear(state_dim * num_classes + action_dim, mlp_hidden_dim)\n","        self.transition = nn.GRUCell(mlp_hidden_dim, rnn_hidden_dim)\n","\n","        # transition predictor\n","        self.prior_hidden = nn.Linear(rnn_hidden_dim, mlp_hidden_dim)\n","        self.prior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n","\n","        # representation model\n","        self.posterior_hidden = nn.Linear(rnn_hidden_dim + 1536, mlp_hidden_dim)\n","        self.posterior_logits = nn.Linear(mlp_hidden_dim, state_dim * num_classes)\n","\n","    def recurrent(self, state: torch.Tensor, action: torch.Tensor, rnn_hidden: torch.Tensor):\n","        # recullent model: h_t = f(h_t-1, z_t-1, a_t-1)を計算する\n","        hidden = F.elu(self.transition_hidden(torch.cat([state, action], dim=1)))\n","        rnn_hidden = self.transition(hidden, rnn_hidden)\n","\n","        return rnn_hidden  # h_t\n","\n","    def get_prior(self, rnn_hidden: torch.Tensor, detach=False):\n","        # transition predictor: \\hat{z}_t ~ p(z\\hat{z}_t | h_t)\n","        hidden = F.elu(self.prior_hidden(rnn_hidden))\n","        logits = self.prior_logits(hidden)\n","        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n","\n","        prior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n","        if detach:\n","            detach_prior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n","            return prior_dist, detach_prior  # p(z\\hat{z}_t | h_t)\n","        return prior_dist\n","\n","    def get_posterior(self, rnn_hidden: torch.Tensor, embedded_obs: torch.Tensor, detach=False):\n","        # representation predictor: z_t ~ q(z_t | h_t, o_t)\n","        hidden = F.elu(self.posterior_hidden(torch.cat([rnn_hidden, embedded_obs], dim=1)))\n","        logits = self.posterior_logits(hidden)\n","        logits = logits.reshape(logits.shape[0], self.state_dim, self.num_classes)\n","\n","        posterior_dist = td.Independent(OneHotCategoricalStraightThrough(logits=logits), 1)\n","        if detach:\n","            detach_posterior = td.Independent(OneHotCategoricalStraightThrough(logits=logits.detach()), 1)\n","            return posterior_dist, detach_posterior  # q(z_t | h_t, o_t)\n","        return posterior_dist\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(3, 48, kernel_size=4, stride=2)\n","        self.conv2 = nn.Conv2d(48, 96, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(96, 192, kernel_size=4, stride=2)\n","        self.conv4 = nn.Conv2d(192, 384, kernel_size=4, stride=2)\n","\n","    def forward(self, obs: torch.Tensor):\n","        \"\"\"\n","        観測画像をベクトルに埋め込むためのEncoder．\n","\n","        Parameters\n","        ----------\n","        obs : torch.Tensor (B, C, H, W)\n","            入力となる観測画像．\n","\n","        Returns\n","        -------\n","        embedded_obs : torch.Tensor (B, D)\n","            観測画像をベクトルに変換したもの．Dは入力画像の幅と高さに依存して変わる．\n","            入力が(B, 3, 64, 64)の場合，出力は(B, 1536)になる．\n","        \"\"\"\n","        hidden = F.elu(self.conv1(obs))\n","        hidden = F.elu(self.conv2(hidden))\n","        hidden = F.elu(self.conv3(hidden))\n","        embedded_obs = self.conv4(hidden).reshape(hidden.size(0), -1)\n","\n","        return embedded_obs  # x_t\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","\n","        self.conv1 = nn.Conv2d(3, 48, kernel_size=4, stride=2)\n","        self.conv2 = nn.Conv2d(48, 96, kernel_size=4, stride=2)\n","        self.conv3 = nn.Conv2d(96, 192, kernel_size=4, stride=2)\n","        self.conv4 = nn.Conv2d(192, 384, kernel_size=4, stride=2)\n","\n","    def forward(self, obs: torch.Tensor):\n","        \"\"\"\n","        観測画像をベクトルに埋め込むためのEncoder．\n","\n","        Parameters\n","        ----------\n","        obs : torch.Tensor (B, C, H, W)\n","            入力となる観測画像．\n","\n","        Returns\n","        -------\n","        embedded_obs : torch.Tensor (B, D)\n","            観測画像をベクトルに変換したもの．Dは入力画像の幅と高さに依存して変わる．\n","            入力が(B, 3, 64, 64)の場合，出力は(B, 1536)になる．\n","        \"\"\"\n","        hidden = F.elu(self.conv1(obs))\n","        hidden = F.elu(self.conv2(hidden))\n","        hidden = F.elu(self.conv3(hidden))\n","        embedded_obs = self.conv4(hidden).reshape(hidden.size(0), -1)\n","\n","        return embedded_obs  # x_t\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","        self.fc = nn.Linear(state_dim*num_classes + rnn_hidden_dim, 1536)\n","        self.dc1 = nn.ConvTranspose2d(1536, 192, kernel_size=5, stride=2)\n","        self.dc2 = nn.ConvTranspose2d(192, 96, kernel_size=5, stride=2)\n","        self.dc3 = nn.ConvTranspose2d(96, 48, kernel_size=6, stride=2)\n","        self.dc4 = nn.ConvTranspose2d(48, 3, kernel_size=6, stride=2)\n","\n","\n","    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        決定論的状態と，確率的状態を入力として，観測画像を復元するDecoder．\n","        出力は多次元正規分布の平均値をとる．\n","\n","        Paremters\n","        ---------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        obs_dist : torch.distribution.Independent\n","            観測画像を再構成するための多次元正規分布．\n","        \"\"\"\n","        hidden = self.fc(torch.cat([state, rnn_hidden], dim=1))\n","        hidden = hidden.view(hidden.size(0), 1536, 1, 1)\n","        hidden = F.elu(self.dc1(hidden))\n","        hidden = F.elu(self.dc2(hidden))\n","        hidden = F.elu(self.dc3(hidden))\n","        mean = self.dc4(hidden)\n","\n","        obs_dist = td.Independent(MSE(mean), 3)\n","        return obs_dist  # p(\\hat{x}_t | h_t, z_t)\n","\n","\n","class RewardModel(nn.Module):\n","    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        決定論的状態と，確率的状態を入力として，報酬を予測するモデル．\n","        出力は正規分布の平均値をとる．\n","\n","        Paremters\n","        ---------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        reward_dist : torch.distribution.Independent\n","            報酬を予測するための正規分布．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        mean = self.fc4(hidden)\n","\n","        reward_dist = td.Independent(MSE(mean),  1)\n","        return reward_dist  # p(\\hat{r}_t | h_t, z_t)\n","\n","\n","class DiscountModel(nn.Module):\n","    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","        self.fc1 = nn.Linear(state_dim*num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state: torch.Tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        決定論的状態と，確率的状態を入力として，現在の状態がエピソード終端かどうか判別するモデル．\n","        出力はベルヌーイ分布の平均値をとる．\n","\n","        Paremters\n","        ---------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        discount_dist : torch.distribution.Independent\n","            状態が終端かどうかを予測するためのベルヌーイ分布．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        mean= self.fc4(hidden)\n","\n","        discount_dist = td.Independent(td.Bernoulli(logits=mean),  1)\n","        return discount_dist  # p(\\hat{\\gamma}_t | h_t, z_t)\n","\n","\n","class Actor(nn.Module):\n","    def __init__(self, action_dim: int, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","\n","        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n","        self.mean = nn.Linear(hidden_dim, action_dim)\n","        self.std = nn.Linear(hidden_dim, action_dim)\n","        self.min_stddev = 0.1\n","        self.init_stddev = np.log(np.exp(5.0) - 1)\n","\n","    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor, eval: bool = False):\n","        \"\"\"\n","        確率的状態を入力として，criticで推定される価値が最大となる行動を出力する．\n","\n","        Parameters\n","        ----------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        action : torch.Tensor (B, 1)\n","            行動．\n","        action_log_prob : torch.Tensor(B, 1)\n","            予測した行動をとる確率の対数．\n","        action_entropy : torch.Tensor(B, 1)\n","            予測した確率分布のエントロピー．エントロピー正則化に使用．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        hidden = F.elu(self.fc4(hidden))\n","        mean = self.mean(hidden)\n","        stddev = self.std(hidden)\n","\n","        mean = torch.tanh(mean)\n","        stddev = 2 * torch.sigmoid((stddev + self.init_stddev) / 2) + self.min_stddev\n","        if eval:\n","            action = mean\n","            return action, None, None\n","\n","        action_dist = td.Independent(TruncNormalDist(mean, stddev, -1, 1), 1)  # 行動をサンプリングする分布: p_{\\psi} (\\hat{a}_t | \\hat{z}_t)\n","        action = action_dist.sample()  # 行動: \\hat{a}_t\n","\n","        action_log_prob = action_dist.log_prob(action)\n","        action_entropy = action_dist.entropy()\n","\n","        return action, action_log_prob, action_entropy\n","\n","\n","class Critic(nn.Module):\n","    def __init__(self, hidden_dim: int, rnn_hidden_dim: int, state_dim: int, num_classes: int):\n","        super().__init__()\n","\n","        self.fc1 = nn.Linear(state_dim * num_classes + rnn_hidden_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc4 = nn.Linear(hidden_dim, hidden_dim)\n","        self.out = nn.Linear(hidden_dim, 1)\n","\n","    def forward(self, state: torch.tensor, rnn_hidden: torch.Tensor):\n","        \"\"\"\n","        確率的状態を入力として，価値関数(lambda target)の値を予測する．．\n","\n","        Parameters\n","        ----------\n","        state : torch.Tensor (B, state_dim * num_classes)\n","            確率的状態．\n","        rnn_hidden : torch.Tensor (B, rnn_hidden_dim)\n","            決定論的状態．\n","\n","        Returns\n","        -------\n","        value : torch.Tensor (B, 1)\n","            入力された状態に対する状態価値関数の予測値．\n","        \"\"\"\n","        hidden = F.elu(self.fc1(torch.cat([state, rnn_hidden], dim=1)))\n","        hidden = F.elu(self.fc2(hidden))\n","        hidden = F.elu(self.fc3(hidden))\n","        hidden = F.elu(self.fc4(hidden))\n","        mean = self.out(hidden)\n","\n","        return mean\n","\n","\n","class Agent(nn.Module):\n","    \"\"\"\n","    ActionModelに基づき行動を決定する. そのためにRSSMを用いて状態表現をリアルタイムで推論して維持するクラス\n","    \"\"\"\n","    def __init__(self, encoder, decoder, rssm, action_model):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.rssm = rssm\n","        self.action_model = action_model\n","\n","        self.device = next(self.action_model.parameters()).device\n","        self.rnn_hidden = torch.zeros(1, rssm.rnn_hidden_dim, device=self.device)\n","\n","    def __call__(self, obs, eval=True):\n","        # preprocessを適用, PyTorchのためにChannel-Firstに変換\n","        obs = preprocess_obs(obs)\n","        obs = torch.as_tensor(obs, device=self.device)\n","        obs = obs.transpose(1, 2).transpose(0, 1).unsqueeze(0)\n","\n","        with torch.no_grad():\n","            # 現在の状態から次に得られる観測画像を予測する\n","            state_prior = self.rssm.get_prior(self.rnn_hidden)\n","            state = state_prior.sample().flatten(1)\n","            obs_dist = self.decoder(state, self.rnn_hidden)\n","            obs_pred = obs_dist.sample()\n","\n","            # 観測を低次元の表現に変換し, posteriorからのサンプルをActionModelに入力して行動を決定する\n","            embedded_obs = self.encoder(obs)\n","            state_posterior = self.rssm.get_posterior(self.rnn_hidden, embedded_obs)\n","            state = state_posterior.sample().flatten(1)\n","            action, _, _  = self.action_model(state, self.rnn_hidden, eval=eval)\n","\n","            # 次のステップのためにRNNの隠れ状態を更新しておく\n","            self.rnn_hidden = self.rssm.recurrent(state, action, self.rnn_hidden)\n","\n","        return action.squeeze().cpu().numpy(), (obs_pred.squeeze().cpu().numpy().transpose(1, 2, 0) + 0.5).clip(0.0, 1.0)\n","\n","    #RNNの隠れ状態をリセット\n","    def reset(self):\n","        self.rnn_hidden = torch.zeros(1, self.rssm.rnn_hidden_dim, device=self.device)\n","\n","    def to(self, device):\n","        self.device = device\n","        self.encoder.to(device)\n","        self.decoder.to(device)\n","        self.rssm.to(device)\n","        self.action_model.to(device)\n","        self.rnn_hidden = self.rnn_hidden.to(device)\n","\n","\n","def preprocess_obs(obs):\n","    \"\"\"\n","    画像の変換. [0, 255] -> [-0.5, 0.5]\n","    \"\"\"\n","    obs = obs.astype(np.float32)\n","    normalized_obs = obs / 255.0 - 0.5\n","    return normalized_obs"]},{"cell_type":"markdown","metadata":{"id":"1nj9x0wgXWb3"},"source":["## 8. submission_tool の準備\n","- submission_tool_colab.zip をアップロードして解凍する．"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"0vNJTgU9Xjy4","colab":{"base_uri":"https://localhost:8080/","height":42},"executionInfo":{"status":"ok","timestamp":1762155750252,"user_tz":-540,"elapsed":5328,"user":{"displayName":"guch1120","userId":"08277471228501584593"}},"outputId":"005459b3-ca41-40b1-9ee2-1f94fadde556"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-2f14f0b3-93a7-4247-a9a5-72c510f50ecd\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-2f14f0b3-93a7-4247-a9a5-72c510f50ecd\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}}],"source":["# google colab の場合はアップロード\n","# ローカルの場合は，ファイルのパスをローカルのパスに変更してください．\n","from google.colab import files\n","import os\n","import zipfile\n","from pathlib import Path\n","\n","uploaded = files.upload()\n","\n","# 解凍\n","with zipfile.ZipFile(\"submission_tool_colab.zip\", \"r\") as zip_ref:\n","    zip_ref.extractall(\".\")"]},{"cell_type":"markdown","metadata":{"id":"9u8Q-aKfXYnT"},"source":["## 9. 提出物の作成\n","- 解凍した submission_tool の中身を利用して提出物を作成する．\n","- コマンドは基本修正しないでください．\n","  - エージェントのパラメータのパスのみ修正しても問題ございません．\n","- `import error` で student_code の import ができない場合， student_code.py が存在していない，もしくはファイル名が間違っている可能性があります．\n","- `Can't get attribute {class name}` のようにご自身で実装したクラス・関数がない場合もエラーが発生します．"]},{"cell_type":"code","execution_count":66,"metadata":{"id":"3Mrp8lTkYV-6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762155902520,"user_tz":-540,"elapsed":149791,"user":{"displayName":"guch1120","userId":"08277471228501584593"}},"outputId":"8f27e36d-66b5-44fd-d74e-c0889ebaedd9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n","Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n","See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n","🔮 Running predictions...\n","/usr/local/lib/python3.12/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","✅ Generated 500 actions and 500 pred obses\n","✅ submission.zip generated - ready to upload\n","✅ Wrapper script created\n"]}],"source":["!python secure_submit.py --student student_code.py --model agent.pth --out submission.zip"]},{"cell_type":"markdown","metadata":{"id":"4gxvcOZIXa2Y"},"source":["## 10. 提出内容のプレビュー"]},{"cell_type":"code","execution_count":67,"metadata":{"id":"12eyqtzIYa4f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1762155905716,"user_tz":-540,"elapsed":23,"user":{"displayName":"guch1120","userId":"08277471228501584593"}},"outputId":"0c92ecbc-6639-4cfb-f922-77942a4dc414"},"outputs":[{"output_type":"stream","name":"stdout","text":["  📄 actions.npy  (7.9 KB)\n","  📄 pred_obses.npy (24,000.1 KB)\n","  📄 sig.txt      (0.1 KB)\n","  📄 agent.pth    (58,281.3 KB)\n"]}],"source":["# 作成された zip ファイルの構成の確認\n","if os.path.exists('submission.zip'):\n","    import zipfile\n","    with zipfile.ZipFile('submission.zip', 'r') as zip_file:\n","        for file_info in zip_file.filelist:\n","            file_size_kb = file_info.file_size / 1024\n","            print(f\"  📄 {file_info.filename:<12} ({file_size_kb:,.1f} KB)\")\n","\n","else:\n","    print(\"Submission file not found\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F1AbEzHOc3Sv"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1Xr8KERREeJ_igEWTvs-0SGUqtXRYmbW1","timestamp":1692732300142}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"},"vscode":{"interpreter":{"hash":"1c906a337007ca492b40f9e66323e61f3dcaf71886120485625fb02da1be1aa9"}},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}