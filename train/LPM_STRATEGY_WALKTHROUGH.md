
# Walkthrough - LPMスイッチング戦略の改善

- [x] LPMスイッチング戦略の議論と策定 (First-Success Triggered Decay -> Reliability-Gated)
- [x] `train/train_lpm_modified.py` への実装
    - $R^2$スコアによる信頼度ゲーティングの実装
    - 直近平均報酬によるEta減衰/回復ロジックの実装
    - コンフィグパラメータの整理と誤字修正

## 実装された戦略: Reliability-Gated & Reward-Feedback

### 1. Reliability-Gated (信頼度ゲーティング) - "賢くなるまで待つ"
ErrorPredictorが未熟なうちは内発的報酬を与えず、予測精度($R^2$)の向上に伴って自動的に探索フェーズへ移行します。

```python
# R2 (信頼度) 計算
ep_variance = torch.var(eps_batch)
ep_r2 = 1.0 - ep_loss_D / ep_variance
ep_reliability = max(0.0, ep_r2) # 負の値は0(信頼なし)とする

# 報酬への適用
eta_current = eta_base * ep_reliability
```

### 2. Reward-Feedback (報酬フィードバック) - "見つけたら集中、見失ったら探索"
直近の成績をもとに、探索の強さ（`eta_base`）を動的に調整します。

```python
# 直近10エピソードの平均報酬
avg_recent_reward = np.mean(recent_ext_rewards)

if avg_recent_reward > 0:
    # 成功中: 集中モード (減衰)
    eta_base = eta_base * 0.995 + 0.1 * (1 - 0.995)
else:
    # 失敗中: 探索モード (回復)
    eta_base = 1.0
```

## 次のステップ
- 実機（Colab等）でのトレーニング実行と、WandBログによる `LPM/ep_reliability` および `LPM/eta_base` の挙動確認。

## 検証環境: Atari Bank Heist について

**目的**: 銀行強盗（赤い車）となり、パトカー（青い車）を避けながら迷路内の銀行（四角いマーク）をすべて襲撃する。

**基本ルール**:
1.  **移動**: 迷路内を走行し、燃料（Fuel）を消費する。燃料切れはミス（残機減少）。
2.  **銀行強盗**: 銀行アイコンを通過するとスコア獲得。
3.  **警察**: 銀行を襲うとパトカーが出現・追跡してくる。接触するとミス。
4.  **攻撃**: ダイナマイトを落としてパトカーを破壊可能（燃料消費）。
5.  **クリア**: 全ての銀行を襲うか、画面端から次の街へ移動すると燃料回復＆ボーナス。

**仕様メモ (学習ログへの影響)**:
*   **点滅（Low Fuel Warning）**: 燃料が少なくなると自機がチカチカ点滅して警告します。評価動画での点滅は学習上の仕様（Frame Skip）に加え、このゲームシステムの警告である可能性があります。
*   **報酬の性質**: 移動中は報酬0、銀行通過時やパトカー撃破時のみプラスの報酬が入るため、学習曲線は常に振動します。

## WandB グラフ変数の解説

`LPM/` および `Train/` セクションに表示される主要な変数の意味は以下の通りです。

| 変数名 | 日本語訳 | 解説 |
| :--- | :--- | :--- |
| `LPM/ep_reliability` | 信頼度 ($R^2$) | ErrorPredictorがどれだけ正確に誤差を予測できているか。`0.0`(デタラメ)〜`1.0`(完璧)の値。これが低いと内発的報酬はゼロになります。 |
| `LPM/eta_current` | 現在の探索係数 | 実際に適用されている内発的報酬の強さ。`eta_base * ep_reliability` で計算されます。「信頼度」が上がるとここも上がり、報酬を見つけると下がります。 |
| `LPM/eta_base` | 基礎探索係数 | 「報酬発見」の状態を表す基礎パラメータ。外的報酬が得られている間は徐々に `0.1` に近づき（探索抑制）、報酬が途切れると `1.0` に戻ります（探索再開）。 |
| `LPM/intrinsic_eta_scaled` | 実質内発報酬 | `eta_current` にスケーリングされた後の、実際に強化学習エージェントに渡される内発的報酬の値。 |
| `LPM/pred_prev_eps` | 予測された誤差 | ErrorPredictorが予測した「次のステップでの驚き」。これが高い未知の状態ほど、探索価値が高いとみなされます。 |
| `Train/Reward_Mean_Ext` | 平均外的報酬 | バッチ内の**外的報酬（Environment Reward）**の平均値。ゲームのスコアそのもの。 |
| `Episode/Extrinsic_Reward` | エピソード報酬 | 1エピソード（ゲームオーバーまで）で獲得したスコアの合計。 |

## 学習曲線の解釈と「成功」の定義について

### Q. 「成功」とは何か？グラフは振動しているが？

**A. 振動しながら右肩上がり（またはベースライン維持）であれば成功です。**

1.  **`eta_base` が下がり続けている意味**:
    *   これは**「エージェントが継続的に何らかの報酬を得続けている」**ことの証明です。もし学習が失敗して報酬が取れなくなれば、ロジックにより `eta_base` は `1.0` へと跳ね上がります（回復）。下がり続けている＝成功状態（Focus Mode）を維持できているということです。

2.  **報酬が振動する理由**:
    *   **Atariの特性**: BankHeistなどは迷路探索要素があり、ステップごとに報酬が一定ではありません。
    *   **探索の残存**: LPMは探索を完全に0にはしません（最小値 `0.1`）。そのため、常に少しの「寄り道（探索）」をしながら学習します。これにより局所解（Local Optima）へのハマりを防いでいます。
    *   **フェーズの循環**: 「新しい部屋を見つける（内発UP）」→「お宝ゲット（外的UP、内発DOWN）」→「取り尽くす（外的DOWN、内発UP）」というサイクルが回っている証拠でもあります。

**結論**:
ログの挙動は、**「無駄なランダム探索をせず（初期）」**、**「報酬源を見つけたらそこに集中し（中期）」**、**「それでも探索を捨てない」**というLPMの設計思想が見事に機能していることを示しており、**アルゴリズムとしては大成功**と言えます。
